/**
 * Direct AI Service - Native Chinese AI Providers
 * Implements true streaming without OpenAI compatibility layers
 * Massive performance improvement through direct provider integration
 */

const winston = require("winston");
const VectorStore = require("./rag/services/vector-store");
const ChineseAIProviders = require("./providers/chinese-ai-providers");
const PerformanceOptimizer = require("./performance-optimization");

const vectorStore = new VectorStore();

// Enhanced logging for direct streaming
const logger = winston.createLogger({
  level: "info",
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.errors({ stack: true }),
    winston.format.json(),
    winston.format.printf(({
      timestamp,
      level,
      message,
      service,
      isDirectStream,
      requestId,
      provider,
      ...meta
    }) => {
      const logData = {
        timestamp,
        level,
        message,
        service,
        isDirectStream: isDirectStream || false,
        provider: provider || 'unknown',
        requestId,
        ...meta,
      };
      return JSON.stringify(logData);
    }),
  ),
  defaultMeta: {
    service: "ai-service-direct",
    isDirectStream: true,
  },
  transports: [
    new winston.transports.Console({
      format: winston.format.combine(
        winston.format.colorize(),
        winston.format.printf(({ timestamp, level, message, isDirectStream, requestId, provider, ...meta }) => {
          const streamFlag = isDirectStream ? "üöÄ[DIRECT]" : "üîß[LEGACY]";
          const providerFlag = provider ? `[${provider.toUpperCase()}]` : "";
          const reqId = requestId ? `[${requestId}]` : "";
          return `${timestamp} ${level} ${streamFlag}${providerFlag}${reqId} ${message} ${Object.keys(meta).length ? JSON.stringify(meta) : ""}`;
        }),
      ),
    }),
    new winston.transports.File({
      filename: "logs/direct-streaming.log",
      level: "info",
      format: winston.format.json(),
    }),
  ],
});

class DirectAIService {
  constructor() {
    // Check required environment variables
    if (!process.env.DASHSCOPE_API_KEY) {
      throw new Error("DASHSCOPE_API_KEY environment variable is required");
    }

    // Initialize request counter
    this.requestCounter = 0;
    
    // Initialize Chinese AI providers
    this.aiProviders = new ChineseAIProviders();
    
    // Initialize performance optimizer
    this.performanceOptimizer = new PerformanceOptimizer();

    // Configuration
    this.model = process.env.QWEN_MODEL || "qwen-turbo";
    this.maxTokens = parseInt(process.env.AI_MAX_TOKENS) || 1500; // Reduced default
    this.temperature = parseFloat(process.env.AI_TEMPERATURE) || 0.7;
    this.topP = parseFloat(process.env.AI_TOP_P) || 0.8;
    this.enabled = process.env.AI_ENABLED !== "false";

    // Speed optimization settings
    this.speedOptimizations = {
      enableDirectStreaming: true,
      enableProgressiveFeedback: true,
      enableSmartCaching: true,
      enableProviderFallback: true,
      timeoutMs: 30000, // 30 second timeout
    };

    logger.info("Direct AI Service initialized", {
      model: this.model,
      maxTokens: this.maxTokens,
      temperature: this.temperature,
      topP: this.topP,
      enabled: this.enabled,
      providers: this.aiProviders.getProvidersStatus(),
      optimizations: this.speedOptimizations,
    });
  }

  /**
   * Generate unique request ID
   */
  generateRequestId() {
    this.requestCounter++;
    const timestamp = new Date()
      .toISOString()
      .replace(/[-:]/g, "")
      .slice(0, 15);
    return `DIRECT-${timestamp}-${this.requestCounter.toString().padStart(4, "0")}`;
  }

  /**
   * Direct streaming lesson plan generation - TRUE STREAMING
   * @param {string} subject Subject
   * @param {string} grade Grade level
   * @param {string} topic Topic
   * @param {string} requirements Requirements
   * @param {Object} res Express response object
   */
  async generateLessonPlanStreamDirect(subject, grade, topic, requirements, res) {
    if (!this.enabled) {
      throw new Error("AI service is disabled");
    }

    const requestId = this.generateRequestId();
    const startTime = Date.now();

    // Check cache first for instant response
    const cached = this.performanceOptimizer.getCachedResponse(
      subject, grade, topic, requirements
    );
    
    if (cached) {
      logger.info("Cache hit - instant response", {
        requestId,
        subject,
        grade,
        topic,
        provider: "cache",
        responseTime: 0
      });
      
      res.setHeader("Content-Type", "text/plain; charset=utf-8");
      res.setHeader("X-Cache", "HIT");
      res.write(cached);
      res.end();
      return;
    }

    const logContext = {
      requestId,
      subject,
      grade,
      topic,
      requirementsLength: requirements?.length || 0,
      startTime: new Date(startTime).toISOString(),
      provider: this.aiProviders.currentProvider,
    };

    try {
      logger.info("Starting direct streaming generation", logContext);

      // Set up direct streaming response headers
      res.setHeader("Content-Type", "text/plain; charset=utf-8");
      res.setHeader("Cache-Control", "no-cache");
      res.setHeader("Connection", "keep-alive");
      res.setHeader("X-Stream-Type", "DIRECT");
      res.setHeader("X-Provider", this.aiProviders.currentProvider);

      // Progressive feedback - immediate response
      if (this.speedOptimizations.enableProgressiveFeedback) {
        res.write("üöÄ AIÂºÄÂßãÁîüÊàêÊïôÊ°à...\n\n");
      }

      // Get RAG context with timeout
      let ragContext = "";
      let ragSources = [];
      
      const ragStartTime = Date.now();
      try {
        const ragResult = await Promise.race([
          vectorStore.getRelevantContext(topic, subject, grade, 1200), // Reduced context
          new Promise((_, reject) => 
            setTimeout(() => reject(new Error("RAG timeout")), 3000) // 3s timeout
          )
        ]);
        
        ragContext = ragResult.context;
        ragSources = ragResult.sources;
        
        const ragTime = Date.now() - ragStartTime;
        logger.info("RAG context retrieved", {
          ...logContext,
          ragTime,
          contextLength: ragContext.length,
          sourcesCount: ragSources.length
        });
      } catch (error) {
        logger.warn("RAG retrieval failed, proceeding without context", {
          ...logContext,
          error: error.message
        });
      }

      // Create optimized prompts
      const systemPrompt = this.createOptimizedSystemPrompt(subject, grade);
      const userPrompt = this.createOptimizedUserPrompt(
        subject, grade, topic, requirements, ragContext
      );

      // Get speed-optimized parameters
      const streamParams = this.getSpeedOptimizedParams("lesson_plan");

      // Create direct stream from Chinese AI provider
      logger.info("Creating direct stream", {
        ...logContext,
        model: streamParams.model,
        maxTokens: streamParams.maxTokens,
        provider: this.aiProviders.currentProvider
      });

      const stream = await this.aiProviders.createStreamCompletion({
        model: streamParams.model,
        messages: [
          { role: "system", content: systemPrompt },
          { role: "user", content: userPrompt }
        ],
        maxTokens: streamParams.maxTokens,
        temperature: streamParams.temperature,
        topP: streamParams.topP,
        stream: true
      });

      let fullContent = "";
      let firstContentReceived = false;
      let tokenCount = 0;

      // Process direct stream - TRUE STREAMING
      for await (const chunk of stream) {
        if (chunk.type === 'content' && chunk.content) {
          // Remove progress indicator when first real content arrives
          if (!firstContentReceived && this.speedOptimizations.enableProgressiveFeedback) {
            res.write("\rüéØ Ê≠£Âú®ÁîüÊàêÔºö\n\n");
            firstContentReceived = true;
          }

          // Write content directly to response - NO BUFFERING
          res.write(chunk.content);
          fullContent += chunk.content;
          tokenCount++;

          // Log progress every 50 tokens
          if (tokenCount % 50 === 0) {
            const currentTime = Date.now() - startTime;
            logger.debug("Streaming progress", {
              ...logContext,
              tokens: tokenCount,
              contentLength: fullContent.length,
              elapsedMs: currentTime,
              tokensPerSecond: (tokenCount / (currentTime / 1000)).toFixed(2)
            });
          }
        } else if (chunk.type === 'completion') {
          // Stream completed
          const endTime = Date.now();
          const duration = endTime - startTime;

          logger.info("Direct streaming completed", {
            ...logContext,
            phase: "completion",
            contentLength: fullContent.length,
            tokens: chunk.usage?.completion_tokens || tokenCount,
            duration: `${duration}ms`,
            tokensPerSecond: chunk.usage?.tokens_per_second || (tokenCount / (duration / 1000)),
            provider: this.aiProviders.currentProvider,
            success: true
          });

          // Cache successful response for future requests
          if (this.speedOptimizations.enableSmartCaching && fullContent.length > 100) {
            this.performanceOptimizer.cacheResponse(
              subject, grade, topic, requirements, fullContent
            );
          }

          break;
        }
      }

      res.end();

      // Record performance metrics
      const finalResponseTime = Date.now() - startTime;
      this.performanceOptimizer.recordMetrics(startTime, false);

      logger.info("Lesson plan generation completed successfully", {
        ...logContext,
        phase: "success",
        finalResponseTime,
        contentLength: fullContent.length,
        cacheUpdated: this.speedOptimizations.enableSmartCaching
      });

    } catch (error) {
      const errorTime = Date.now() - startTime;
      
      logger.error("Direct streaming failed", {
        ...logContext,
        phase: "error",
        error: error.message,
        errorStack: error.stack,
        duration: `${errorTime}ms`,
        provider: this.aiProviders.currentProvider
      });

      // Fallback strategy
      if (this.speedOptimizations.enableProviderFallback) {
        await this.handleStreamingFallback(error, res, logContext);
      } else {
        if (!res.headersSent) {
          res.status(500);
        }
        res.write(`\n‚ùå ÁîüÊàêÂ§±Ë¥•: ${error.message}`);
        res.end();
      }
    }
  }

  /**
   * Handle streaming fallback when primary provider fails
   */
  async handleStreamingFallback(primaryError, res, logContext) {
    logger.info("Attempting fallback strategy", logContext);

    try {
      // Try basic response generation
      const fallbackContent = this.generateFallbackContent(
        logContext.subject,
        logContext.grade,
        logContext.topic
      );

      if (!res.headersSent) {
        res.setHeader("X-Fallback", "true");
      }
      
      res.write("\n\n‚ö†Ô∏è ‰ΩøÁî®Â§áÁî®Ê®°ÂºèÁîüÊàêÔºö\n\n");
      res.write(fallbackContent);
      res.end();

      logger.info("Fallback response sent", {
        ...logContext,
        fallbackContentLength: fallbackContent.length
      });

    } catch (fallbackError) {
      logger.error("Fallback also failed", {
        ...logContext,
        primaryError: primaryError.message,
        fallbackError: fallbackError.message
      });

      if (!res.headersSent) {
        res.status(500);
      }
      res.write(`\n‚ùå ÊâÄÊúâÁîüÊàêÊñπÂºèÂùáÂ§±Ë¥•ÔºåËØ∑Á®çÂêéÈáçËØï`);
      res.end();
    }
  }

  /**
   * Generate fallback content
   */
  generateFallbackContent(subject, grade, topic) {
    return `# ${topic} - ${subject}ÊïôÊ°à

**Âπ¥Á∫ß**: ${grade}
**ÁßëÁõÆ**: ${subject}
**ËØæÈ¢ò**: ${topic}

## üéØ ÊïôÂ≠¶ÁõÆÊ†á
- ÊéåÊè°${topic}ÁöÑÂü∫Êú¨Ê¶ÇÂøµÂíåÂéüÁêÜ
- ËÉΩÂ§üËøêÁî®${topic}Áü•ËØÜËß£ÂÜ≥ÂÆûÈôÖÈóÆÈ¢ò
- ÂüπÂÖªÂ≠¶ÁîüÁöÑÂ≠¶‰π†ÂÖ¥Ë∂£ÂíåÊÄùÁª¥ËÉΩÂäõ

## üìö ÊïôÂ≠¶ÈáçÁÇπ
- ${topic}ÁöÑÊ†∏ÂøÉÊ¶ÇÂøµ
- Âü∫Êú¨ÊñπÊ≥ïÂíåÊäÄËÉΩ
- ÂÆûÈôÖÂ∫îÁî®

## üîç ÊïôÂ≠¶ÈöæÁÇπ  
- Ê¶ÇÂøµÁöÑÊ∑±ÂÖ•ÁêÜËß£
- ÊñπÊ≥ïÁöÑÁÅµÊ¥ªËøêÁî®
- Áü•ËØÜÁöÑËøÅÁßªÂ∫îÁî®

## üìñ ÊïôÂ≠¶ËøáÁ®ã

### 1. ÂØºÂÖ•Êñ∞ËØæ (5ÂàÜÈíü)
- Â§ç‰π†Áõ∏ÂÖ≥Áü•ËØÜ
- ÂºïÂÖ•Êñ∞ËØæ‰∏ªÈ¢ò

### 2. Êñ∞ËØæËÆ≤Ëß£ (25ÂàÜÈíü)
- ËÆ≤Ëß£${topic}ÁöÑÂü∫Êú¨Ê¶ÇÂøµ
- ÊºîÁ§∫Áõ∏ÂÖ≥ÊñπÊ≥ïÂíåÊäÄËÉΩ
- ‰∏æ‰æãËØ¥ÊòéÂ∫îÁî®Âú∫ÊôØ

### 3. ËØæÂ†ÇÁªÉ‰π† (10ÂàÜÈíü)
- Â≠¶ÁîüÁã¨Á´ãÁªÉ‰π†
- ÊïôÂ∏àÂ∑°ËßÜÊåáÂØº

### 4. ÊÄªÁªì‰Ωú‰∏ö (5ÂàÜÈíü)
- ÊÄªÁªìÈáçÁÇπÂÜÖÂÆπ
- Â∏ÉÁΩÆËØæÂêé‰Ωú‰∏ö

## üìù ËØæÂêé‰Ωú‰∏ö
- ÂÆåÊàêÁõ∏ÂÖ≥ÁªÉ‰π†È¢ò
- È¢Ñ‰π†‰∏ãËäÇËØæÂÜÖÂÆπ

## üí≠ ÊïôÂ≠¶ÂèçÊÄù
Êú¨ËäÇËØæÈÄöËøáÁ≥ªÁªüËÆ≤Ëß£ÂíåÂÆûË∑µÁªÉ‰π†ÔºåÂ∏ÆÂä©Â≠¶ÁîüÊéåÊè°${topic}ÁöÑÂü∫Êú¨Áü•ËØÜÂíåÊäÄËÉΩ„ÄÇ

---
‚ö° ËøôÊòØ‰∏Ä‰∏™Âü∫Á°ÄÊïôÊ°àÊ®°ÊùøÔºåÂª∫ËÆÆÊ†πÊçÆÂÖ∑‰ΩìÊïôÂ≠¶ÈúÄÊ±ÇËøõË°åË∞ÉÊï¥ÂíåÂÆåÂñÑ„ÄÇ
`;
  }

  /**
   * Create optimized system prompt for faster generation
   */
  createOptimizedSystemPrompt(subject, grade) {
    return `‰Ω†ÊòØ‰∏ì‰∏öÁöÑ${subject}ÊïôÂ∏àÔºå‰∏∫${grade}Â≠¶ÁîüËÆæËÆ°ÊïôÊ°à„ÄÇ

Ë¶ÅÊ±ÇÔºö
1. ÁîüÊàêÁªìÊûÑÂåñÁöÑÊïôÊ°àÔºåÂåÖÂê´YAMLÂâçÁΩÆÊï∞ÊçÆÂíåMarkdownÊ≠£Êñá
2. ÂÜÖÂÆπË¶ÅÈÄÇÂêà${grade}Â≠¶ÁîüÁöÑËÆ§Áü•Ê∞¥Âπ≥
3. ÈáçÁÇπÁ™ÅÂá∫ÔºåÈöæÁÇπÊòéÁ°ÆÔºåÊñπÊ≥ïÂÆûÁî®
4. Â≠óÊï∞ÊéßÂà∂Âú®800-1200Â≠óÔºåÈÅøÂÖçÂÜóÈïø

ËæìÂá∫Ê†ºÂºèÔºö
---
title: "ËØæÈ¢òÂêçÁß∞"
subject: "${subject}"
grade: "${grade}"
duration: 45
objectives: ["ÁõÆÊ†á1", "ÁõÆÊ†á2"]
---

# MarkdownÊ†ºÂºèÁöÑÊïôÊ°àÊ≠£Êñá`;
  }

  /**
   * Create optimized user prompt
   */
  createOptimizedUserPrompt(subject, grade, topic, requirements, ragContext) {
    let prompt = `ËØ∑‰∏∫${grade}${subject}ËØæÁ®ãËÆæËÆ°„Ää${topic}„ÄãÁöÑÊïôÊ°à„ÄÇ`;
    
    if (requirements) {
      prompt += `\n\nÁâπÂà´Ë¶ÅÊ±ÇÔºö${requirements}`;
    }

    if (ragContext && ragContext.length > 0) {
      // Limit RAG context to prevent overly long prompts
      const limitedContext = ragContext.substring(0, 800);
      prompt += `\n\nÂèÇËÄÉÊùêÊñôÔºö${limitedContext}`;
    }

    return prompt;
  }

  /**
   * Get speed-optimized parameters for different request types
   */
  getSpeedOptimizedParams(requestType) {
    const optimizations = {
      lesson_plan: {
        model: "qwen-turbo", // Fastest model
        maxTokens: Math.min(this.maxTokens, 1200), // Reduced for speed
        temperature: Math.min(this.temperature + 0.1, 1.0), // Slightly higher for speed
        topP: Math.max(this.topP - 0.1, 0.7), // More focused
      },
      exercises: {
        model: "qwen-turbo",
        maxTokens: Math.min(this.maxTokens, 1000),
        temperature: 0.6,
        topP: 0.8,
      },
      analysis: {
        model: "qwen-turbo",
        maxTokens: 200,
        temperature: 0.1,
        topP: 0.6,
      }
    };

    return {
      model: this.model,
      maxTokens: this.maxTokens,
      temperature: this.temperature,
      topP: this.topP,
      ...optimizations[requestType]
    };
  }

  /**
   * Exercise generation with direct streaming
   */
  async generateExercisesStreamDirect(subject, grade, topic, difficulty, count, type, requirements, res) {
    // Similar implementation to lesson plan but optimized for exercises
    // Implementation would be similar but with exercise-specific prompts and optimizations
    
    logger.info("Exercise generation not yet implemented in direct service", {
      subject, grade, topic, difficulty, count, type
    });
    
    res.status(501);
    res.write("Exercise generation will be implemented in direct service soon");
    res.end();
  }

  /**
   * Get service status
   */
  getStatus() {
    return {
      enabled: this.enabled,
      model: this.model,
      maxTokens: this.maxTokens,
      temperature: this.temperature,
      topP: this.topP,
      providers: this.aiProviders.getProvidersStatus(),
      optimizations: this.speedOptimizations,
      performance: this.performanceOptimizer.getPerformanceReport(),
      streaming: "DIRECT", // True direct streaming
      version: "2.0-direct"
    };
  }

  /**
   * Test direct streaming performance
   */
  async testDirectStreaming() {
    const testResults = {
      providers: await this.aiProviders.testAllProviders(),
      performance: this.performanceOptimizer.getPerformanceReport(),
      streaming: "DIRECT_NATIVE"
    };

    return testResults;
  }
}

module.exports = DirectAIService;