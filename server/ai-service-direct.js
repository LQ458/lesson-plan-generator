/**
 * Direct AI Service - Native Chinese AI Providers
 * Implements true streaming without OpenAI compatibility layers
 * Massive performance improvement through direct provider integration
 */

const winston = require("winston");
const SimpleRAGService = require("./rag/services/simple-rag-service");
const ChineseAIProviders = require("./providers/chinese-ai-providers");
const PerformanceOptimizer = require("./performance-optimization");

const simpleRAG = new SimpleRAGService();

// Enhanced logging for direct streaming
const logger = winston.createLogger({
  level: "info",
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.errors({ stack: true }),
    winston.format.json(),
    winston.format.printf(({
      timestamp,
      level,
      message,
      service,
      isDirectStream,
      requestId,
      provider,
      ...meta
    }) => {
      const logData = {
        timestamp,
        level,
        message,
        service,
        isDirectStream: isDirectStream || false,
        provider: provider || 'unknown',
        requestId,
        ...meta,
      };
      return JSON.stringify(logData);
    }),
  ),
  defaultMeta: {
    service: "ai-service-direct",
    isDirectStream: true,
  },
  transports: [
    new winston.transports.Console({
      format: winston.format.combine(
        winston.format.colorize(),
        winston.format.printf(({ timestamp, level, message, isDirectStream, requestId, provider, ...meta }) => {
          const streamFlag = isDirectStream ? "ðŸš€[DIRECT]" : "ðŸ”§[LEGACY]";
          const providerFlag = provider ? `[${provider.toUpperCase()}]` : "";
          const reqId = requestId ? `[${requestId}]` : "";
          return `${timestamp} ${level} ${streamFlag}${providerFlag}${reqId} ${message} ${Object.keys(meta).length ? JSON.stringify(meta) : ""}`;
        }),
      ),
    }),
    new winston.transports.File({
      filename: "logs/direct-streaming.log",
      level: "info",
      format: winston.format.json(),
    }),
  ],
});

class DirectAIService {
  constructor() {
    // Check required environment variables
    if (!process.env.DASHSCOPE_API_KEY) {
      throw new Error("DASHSCOPE_API_KEY environment variable is required");
    }

    // Initialize request counter
    this.requestCounter = 0;
    
    // Initialize Chinese AI providers
    this.aiProviders = new ChineseAIProviders();
    
    // Initialize performance optimizer
    this.performanceOptimizer = new PerformanceOptimizer();

    // Configuration
    this.model = process.env.QWEN_MODEL || "qwen-turbo";
    this.maxTokens = parseInt(process.env.AI_MAX_TOKENS) || 1500; // Reduced default
    this.temperature = parseFloat(process.env.AI_TEMPERATURE) || 0.7;
    this.topP = parseFloat(process.env.AI_TOP_P) || 0.8;
    this.enabled = process.env.AI_ENABLED !== "false";

    // Speed optimization settings
    this.speedOptimizations = {
      enableDirectStreaming: true,
      enableProgressiveFeedback: true,
      enableSmartCaching: true,
      enableProviderFallback: true,
      timeoutMs: 30000, // 30 second timeout
    };

    logger.info("Direct AI Service initialized", {
      model: this.model,
      maxTokens: this.maxTokens,
      temperature: this.temperature,
      topP: this.topP,
      enabled: this.enabled,
      providers: this.aiProviders.getProvidersStatus(),
      optimizations: this.speedOptimizations,
    });
  }

  /**
   * Generate unique request ID
   */
  generateRequestId() {
    this.requestCounter++;
    const timestamp = new Date()
      .toISOString()
      .replace(/[-:]/g, "")
      .slice(0, 15);
    return `DIRECT-${timestamp}-${this.requestCounter.toString().padStart(4, "0")}`;
  }

  /**
   * Direct streaming lesson plan generation - TRUE STREAMING
   * @param {string} subject Subject
   * @param {string} grade Grade level
   * @param {string} topic Topic
   * @param {string} requirements Requirements
   * @param {Object} res Express response object
   */
  async generateLessonPlanStreamDirect(subject, grade, topic, requirements, res) {
    if (!this.enabled) {
      throw new Error("AI service is disabled");
    }

    const requestId = this.generateRequestId();
    const startTime = Date.now();

    // Check cache first for instant response
    const cached = this.performanceOptimizer.getCachedResponse(
      subject, grade, topic, requirements
    );
    
    if (cached) {
      logger.info("Cache hit - instant response", {
        requestId,
        subject,
        grade,
        topic,
        provider: "cache",
        responseTime: 0
      });
      
      res.setHeader("Content-Type", "text/plain; charset=utf-8");
      res.setHeader("X-Cache", "HIT");
      res.write(cached);
      res.end();
      return;
    }

    const logContext = {
      requestId,
      subject,
      grade,
      topic,
      requirementsLength: requirements?.length || 0,
      startTime: new Date(startTime).toISOString(),
      provider: this.aiProviders.currentProvider,
    };

    try {
      logger.info("Starting direct streaming generation", logContext);

      // Set up direct streaming response headers
      res.setHeader("Content-Type", "text/plain; charset=utf-8");
      res.setHeader("Cache-Control", "no-cache");
      res.setHeader("Connection", "keep-alive");
      res.setHeader("X-Stream-Type", "DIRECT");
      res.setHeader("X-Provider", this.aiProviders.currentProvider);

      // Progressive feedback - immediate response
      if (this.speedOptimizations.enableProgressiveFeedback) {
        res.write("ðŸš€ AIå¼€å§‹ç”Ÿæˆæ•™æ¡ˆ...\n\n");
      }

      // Get RAG context with timeout
      let ragContext = "";
      let ragSources = [];
      
      const ragStartTime = Date.now();
      try {
        // Initialize simple RAG if needed
        await simpleRAG.initialize();
        
        const ragResult = await Promise.race([
          this.getRelevantContextSimple(topic, subject, grade, 1200),
          new Promise((_, reject) => 
            setTimeout(() => reject(new Error("RAG timeout")), 3000) // 3s timeout
          )
        ]);
        
        ragContext = ragResult.context;
        ragSources = ragResult.sources;
        
        const ragTime = Date.now() - ragStartTime;
        logger.info("RAG context retrieved", {
          ...logContext,
          ragTime,
          contextLength: ragContext.length,
          sourcesCount: ragSources.length
        });
      } catch (error) {
        logger.warn("RAG retrieval failed, proceeding without context", {
          ...logContext,
          error: error.message
        });
      }

      // Create optimized prompts
      const systemPrompt = this.createOptimizedSystemPrompt(subject, grade);
      const userPrompt = this.createOptimizedUserPrompt(
        subject, grade, topic, requirements, ragContext
      );

      // Get speed-optimized parameters
      const streamParams = this.getSpeedOptimizedParams("lesson_plan");

      // Create direct stream from Chinese AI provider
      logger.info("Creating direct stream", {
        ...logContext,
        model: streamParams.model,
        maxTokens: streamParams.maxTokens,
        provider: this.aiProviders.currentProvider
      });

      const stream = await this.aiProviders.createStreamCompletion({
        model: streamParams.model,
        messages: [
          { role: "system", content: systemPrompt },
          { role: "user", content: userPrompt }
        ],
        maxTokens: streamParams.maxTokens,
        temperature: streamParams.temperature,
        topP: streamParams.topP,
        stream: true
      });

      let fullContent = "";
      let firstContentReceived = false;
      let tokenCount = 0;

      // Process direct stream - TRUE STREAMING
      for await (const chunk of stream) {
        if (chunk.type === 'content' && chunk.content) {
          // Remove progress indicator when first real content arrives
          if (!firstContentReceived && this.speedOptimizations.enableProgressiveFeedback) {
            res.write("\rðŸŽ¯ æ­£åœ¨ç”Ÿæˆï¼š\n\n");
            firstContentReceived = true;
          }

          // Write content directly to response - NO BUFFERING
          res.write(chunk.content);
          fullContent += chunk.content;
          tokenCount++;

          // Log progress every 50 tokens
          if (tokenCount % 50 === 0) {
            const currentTime = Date.now() - startTime;
            logger.debug("Streaming progress", {
              ...logContext,
              tokens: tokenCount,
              contentLength: fullContent.length,
              elapsedMs: currentTime,
              tokensPerSecond: (tokenCount / (currentTime / 1000)).toFixed(2)
            });
          }
        } else if (chunk.type === 'completion') {
          // Stream completed
          const endTime = Date.now();
          const duration = endTime - startTime;

          logger.info("Direct streaming completed", {
            ...logContext,
            phase: "completion",
            contentLength: fullContent.length,
            tokens: chunk.usage?.completion_tokens || tokenCount,
            duration: `${duration}ms`,
            tokensPerSecond: chunk.usage?.tokens_per_second || (tokenCount / (duration / 1000)),
            provider: this.aiProviders.currentProvider,
            success: true
          });

          // Cache successful response for future requests
          if (this.speedOptimizations.enableSmartCaching && fullContent.length > 100) {
            this.performanceOptimizer.cacheResponse(
              subject, grade, topic, requirements, fullContent
            );
          }

          break;
        }
      }

      res.end();

      // Record performance metrics
      const finalResponseTime = Date.now() - startTime;
      this.performanceOptimizer.recordMetrics(startTime, false);

      logger.info("Lesson plan generation completed successfully", {
        ...logContext,
        phase: "success",
        finalResponseTime,
        contentLength: fullContent.length,
        cacheUpdated: this.speedOptimizations.enableSmartCaching
      });

    } catch (error) {
      const errorTime = Date.now() - startTime;
      
      logger.error("Direct streaming failed", {
        ...logContext,
        phase: "error",
        error: error.message,
        errorStack: error.stack,
        duration: `${errorTime}ms`,
        provider: this.aiProviders.currentProvider
      });

      // Fallback strategy
      if (this.speedOptimizations.enableProviderFallback) {
        await this.handleStreamingFallback(error, res, logContext);
      } else {
        if (!res.headersSent) {
          res.status(500);
        }
        res.write(`\nâŒ ç”Ÿæˆå¤±è´¥: ${error.message}`);
        res.end();
      }
    }
  }

  /**
   * Handle streaming fallback when primary provider fails
   */
  async handleStreamingFallback(primaryError, res, logContext) {
    logger.info("Attempting fallback strategy", logContext);

    try {
      // Try basic response generation
      const fallbackContent = this.generateFallbackContent(
        logContext.subject,
        logContext.grade,
        logContext.topic
      );

      if (!res.headersSent) {
        res.setHeader("X-Fallback", "true");
      }
      
      res.write("\n\nâš ï¸ ä½¿ç”¨å¤‡ç”¨æ¨¡å¼ç”Ÿæˆï¼š\n\n");
      res.write(fallbackContent);
      res.end();

      logger.info("Fallback response sent", {
        ...logContext,
        fallbackContentLength: fallbackContent.length
      });

    } catch (fallbackError) {
      logger.error("Fallback also failed", {
        ...logContext,
        primaryError: primaryError.message,
        fallbackError: fallbackError.message
      });

      if (!res.headersSent) {
        res.status(500);
      }
      res.write(`\nâŒ æ‰€æœ‰ç”Ÿæˆæ–¹å¼å‡å¤±è´¥ï¼Œè¯·ç¨åŽé‡è¯•`);
      res.end();
    }
  }

  /**
   * Generate fallback content
   */
  generateFallbackContent(subject, grade, topic) {
    return `# ${topic} - ${subject}æ•™æ¡ˆ

**å¹´çº§**: ${grade}
**ç§‘ç›®**: ${subject}
**è¯¾é¢˜**: ${topic}

## ðŸŽ¯ æ•™å­¦ç›®æ ‡
- æŽŒæ¡${topic}çš„åŸºæœ¬æ¦‚å¿µå’ŒåŽŸç†
- èƒ½å¤Ÿè¿ç”¨${topic}çŸ¥è¯†è§£å†³å®žé™…é—®é¢˜
- åŸ¹å…»å­¦ç”Ÿçš„å­¦ä¹ å…´è¶£å’Œæ€ç»´èƒ½åŠ›

## ðŸ“š æ•™å­¦é‡ç‚¹
- ${topic}çš„æ ¸å¿ƒæ¦‚å¿µ
- åŸºæœ¬æ–¹æ³•å’ŒæŠ€èƒ½
- å®žé™…åº”ç”¨

## ðŸ” æ•™å­¦éš¾ç‚¹  
- æ¦‚å¿µçš„æ·±å…¥ç†è§£
- æ–¹æ³•çš„çµæ´»è¿ç”¨
- çŸ¥è¯†çš„è¿ç§»åº”ç”¨

## ðŸ“– æ•™å­¦è¿‡ç¨‹

### 1. å¯¼å…¥æ–°è¯¾ (5åˆ†é’Ÿ)
- å¤ä¹ ç›¸å…³çŸ¥è¯†
- å¼•å…¥æ–°è¯¾ä¸»é¢˜

### 2. æ–°è¯¾è®²è§£ (25åˆ†é’Ÿ)
- è®²è§£${topic}çš„åŸºæœ¬æ¦‚å¿µ
- æ¼”ç¤ºç›¸å…³æ–¹æ³•å’ŒæŠ€èƒ½
- ä¸¾ä¾‹è¯´æ˜Žåº”ç”¨åœºæ™¯

### 3. è¯¾å ‚ç»ƒä¹  (10åˆ†é’Ÿ)
- å­¦ç”Ÿç‹¬ç«‹ç»ƒä¹ 
- æ•™å¸ˆå·¡è§†æŒ‡å¯¼

### 4. æ€»ç»“ä½œä¸š (5åˆ†é’Ÿ)
- æ€»ç»“é‡ç‚¹å†…å®¹
- å¸ƒç½®è¯¾åŽä½œä¸š

## ðŸ“ è¯¾åŽä½œä¸š
- å®Œæˆç›¸å…³ç»ƒä¹ é¢˜
- é¢„ä¹ ä¸‹èŠ‚è¯¾å†…å®¹

## ðŸ’­ æ•™å­¦åæ€
æœ¬èŠ‚è¯¾é€šè¿‡ç³»ç»Ÿè®²è§£å’Œå®žè·µç»ƒä¹ ï¼Œå¸®åŠ©å­¦ç”ŸæŽŒæ¡${topic}çš„åŸºæœ¬çŸ¥è¯†å’ŒæŠ€èƒ½ã€‚

---
âš¡ è¿™æ˜¯ä¸€ä¸ªåŸºç¡€æ•™æ¡ˆæ¨¡æ¿ï¼Œå»ºè®®æ ¹æ®å…·ä½“æ•™å­¦éœ€æ±‚è¿›è¡Œè°ƒæ•´å’Œå®Œå–„ã€‚
`;
  }

  /**
   * Create optimized system prompt for faster generation
   */
  createOptimizedSystemPrompt(subject, grade) {
    return `ä½ æ˜¯ä¸“ä¸šçš„${subject}æ•™å¸ˆï¼Œä¸º${grade}å­¦ç”Ÿè®¾è®¡æ•™æ¡ˆã€‚

è¦æ±‚ï¼š
1. ç”Ÿæˆç»“æž„åŒ–çš„æ•™æ¡ˆï¼ŒåŒ…å«YAMLå‰ç½®æ•°æ®å’ŒMarkdownæ­£æ–‡
2. å†…å®¹è¦é€‚åˆ${grade}å­¦ç”Ÿçš„è®¤çŸ¥æ°´å¹³
3. é‡ç‚¹çªå‡ºï¼Œéš¾ç‚¹æ˜Žç¡®ï¼Œæ–¹æ³•å®žç”¨
4. å­—æ•°æŽ§åˆ¶åœ¨800-1200å­—ï¼Œé¿å…å†—é•¿

è¾“å‡ºæ ¼å¼ï¼š
---
title: "è¯¾é¢˜åç§°"
subject: "${subject}"
grade: "${grade}"
duration: 45
objectives: ["ç›®æ ‡1", "ç›®æ ‡2"]
---

# Markdownæ ¼å¼çš„æ•™æ¡ˆæ­£æ–‡`;
  }

  /**
   * Create optimized user prompt
   */
  createOptimizedUserPrompt(subject, grade, topic, requirements, ragContext) {
    let prompt = `è¯·ä¸º${grade}${subject}è¯¾ç¨‹è®¾è®¡ã€Š${topic}ã€‹çš„æ•™æ¡ˆã€‚`;
    
    if (requirements) {
      prompt += `\n\nç‰¹åˆ«è¦æ±‚ï¼š${requirements}`;
    }

    if (ragContext && ragContext.length > 0) {
      // Limit RAG context to prevent overly long prompts
      const limitedContext = ragContext.substring(0, 800);
      prompt += `\n\nå‚è€ƒææ–™ï¼š${limitedContext}`;
    }

    return prompt;
  }

  /**
   * Get speed-optimized parameters for different request types
   */
  getSpeedOptimizedParams(requestType) {
    const optimizations = {
      lesson_plan: {
        model: "qwen-turbo", // Fastest model
        maxTokens: Math.min(this.maxTokens, 1200), // Reduced for speed
        temperature: Math.min(this.temperature + 0.1, 1.0), // Slightly higher for speed
        topP: Math.max(this.topP - 0.1, 0.7), // More focused
      },
      exercises: {
        model: "qwen-turbo",
        maxTokens: Math.min(this.maxTokens, 1000),
        temperature: 0.6,
        topP: 0.8,
      },
      analysis: {
        model: "qwen-turbo",
        maxTokens: 200,
        temperature: 0.1,
        topP: 0.6,
      }
    };

    return {
      model: this.model,
      maxTokens: this.maxTokens,
      temperature: this.temperature,
      topP: this.topP,
      ...optimizations[requestType]
    };
  }

  /**
   * Exercise generation with direct streaming
   */
  async generateExercisesStreamDirect(subject, grade, topic, difficulty, count, type, requirements, res) {
    // Similar implementation to lesson plan but optimized for exercises
    // Implementation would be similar but with exercise-specific prompts and optimizations
    
    logger.info("Exercise generation not yet implemented in direct service", {
      subject, grade, topic, difficulty, count, type
    });
    
    res.status(501);
    res.write("Exercise generation will be implemented in direct service soon");
    res.end();
  }

  /**
   * Get service status
   */
  getStatus() {
    return {
      enabled: this.enabled,
      model: this.model,
      maxTokens: this.maxTokens,
      temperature: this.temperature,
      topP: this.topP,
      providers: this.aiProviders.getProvidersStatus(),
      optimizations: this.speedOptimizations,
      performance: this.performanceOptimizer.getPerformanceReport(),
      streaming: "DIRECT", // True direct streaming
      version: "2.0-direct"
    };
  }

  /**
   * Get relevant context using simple RAG service
   */
  async getRelevantContextSimple(topic, subject, grade, maxLength = 1200) {
    try {
      const searchQuery = `${topic} ${subject} ${grade}`;
      const results = await simpleRAG.searchRelevantContent(searchQuery, {
        maxResults: 3,
        subjects: [subject],
        grades: [grade]
      });

      if (!results || results.length === 0) {
        return { context: "", sources: [] };
      }

      let context = "";
      const sources = [];

      for (const result of results) {
        if (context.length + result.content.length > maxLength) {
          break;
        }
        
        context += result.content + "\n\n";
        sources.push({
          source: result.metadata.source,
          grade: result.metadata.grade,
          subject: result.metadata.subject,
          score: result.score
        });
      }

      return {
        context: context.trim(),
        sources: sources
      };

    } catch (error) {
      console.warn('Simple RAG search failed:', error.message);
      return { context: "", sources: [] };
    }
  }

  /**
   * Test direct streaming performance
   */
  async testDirectStreaming() {
    const testResults = {
      providers: await this.aiProviders.testAllProviders(),
      performance: this.performanceOptimizer.getPerformanceReport(),
      streaming: "DIRECT_NATIVE",
      rag: await simpleRAG.getStatus()
    };

    return testResults;
  }
}

module.exports = DirectAIService;