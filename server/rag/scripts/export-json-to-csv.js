/**
 * Export RAG data from JSON files to CSV files organized by book
 * Each book gets its own CSV file with chunks and metadata
 * This approach bypasses ChromaDB and works directly with the JSON files
 */

const fs = require('fs').promises;
const path = require('path');
const createCsvWriter = require('csv-writer').createObjectCsvWriter;

class JSONToCSVExporter {
  constructor() {
    this.inputDir = path.join(__dirname, '..', '..', 'rag_data', 'chunks');
    this.outputDir = path.join(__dirname, '..', 'exported_csv');
  }

  async initialize() {
    try {
      // Create output directory
      await fs.mkdir(this.outputDir, { recursive: true });
      console.log(`âœ… Output directory created: ${this.outputDir}`);
      
      return true;
    } catch (error) {
      console.error(`âŒ Initialization failed:`, error.message);
      throw error;
    }
  }

  /**
   * Extract book name from filename
   */
  extractBookName(filename) {
    if (!filename) return 'unknown';
    
    // Remove file extension
    let bookName = path.basename(filename, '.json');
    
    // Clean up filename - remove hash suffixes and special characters
    bookName = bookName
      .replace(/_[a-f0-9]{8}$/, '') // Remove 8-char hash suffixes like "_1139e269"
      .replace(/[^\u4e00-\u9fa5a-zA-Z0-9Â·ï¼ˆï¼‰()Â·\-]/g, '_') // Replace special chars
      .replace(/_+/g, '_') // Collapse multiple underscores
      .replace(/^_|_$/g, ''); // Remove leading/trailing underscores
    
    return bookName || 'unknown';
  }

  /**
   * Parse metadata to extract educational information
   */
  parseEducationalMetadata(filename, metadata) {
    const parsed = {
      source_file: filename,
      book_name: this.extractBookName(filename),
      subject: this.extractSubject(filename),
      grade: this.extractGrade(filename),
      semester: this.extractSemester(filename),
      publisher: this.extractPublisher(filename),
      // Original metadata fields
      ...metadata
    };

    return parsed;
  }

  extractSubject(filename) {
    const subjects = ['æ•°å­¦', 'è¯­æ–‡', 'è‹±è¯­', 'ç‰©ç†', 'åŒ–å­¦', 'ç”Ÿç‰©', 'åŽ†å²', 'åœ°ç†', 'æ”¿æ²»', 'ç¾Žæœ¯', 'éŸ³ä¹', 'ä½“è‚²', 'ç§‘å­¦', 'é“å¾·ä¸Žæ³•æ²»', 'ä¹¦æ³•', 'ä¿„è¯­', 'æ—¥è¯­'];
    return subjects.find(s => filename.includes(s)) || 'unknown';
  }

  extractGrade(filename) {
    const gradeMatch = filename.match(/(ä¸€å¹´çº§|äºŒå¹´çº§|ä¸‰å¹´çº§|å››å¹´çº§|äº”å¹´çº§|å…­å¹´çº§|ä¸ƒå¹´çº§|å…«å¹´çº§|ä¹å¹´çº§|é«˜ä¸€|é«˜äºŒ|é«˜ä¸‰)/);
    return gradeMatch ? gradeMatch[1] : 'unknown';
  }

  extractSemester(filename) {
    if (filename.includes('ä¸Šå†Œ')) return 'ä¸Šå†Œ';
    if (filename.includes('ä¸‹å†Œ')) return 'ä¸‹å†Œ';
    if (filename.includes('å…¨ä¸€å†Œ')) return 'å…¨ä¸€å†Œ';
    return 'unknown';
  }

  extractPublisher(filename) {
    const publisherPatterns = [
      /äººæ•™ç‰ˆ/,
      /åŒ—å¸ˆå¤§ç‰ˆ/,
      /è‹æ•™ç‰ˆ/,
      /é²æ•™ç‰ˆ/,
      /é’å²›ç‰ˆ/,
      /æ¹˜æ•™ç‰ˆ/,
      /ä¸­å›¾ç‰ˆ/,
      /äººæ°‘æ•™è‚²å‡ºç‰ˆç¤¾/,
      /åŒ—äº¬å¸ˆèŒƒå¤§å­¦å‡ºç‰ˆç¤¾/,
      /æ±Ÿè‹å‡¤å‡°æ•™è‚²å‡ºç‰ˆç¤¾/
    ];
    
    for (const pattern of publisherPatterns) {
      const match = filename.match(pattern);
      if (match) return match[0];
    }
    
    // Extract publisher from parentheses like "ï¼ˆä¸»ç¼–ï¼šçŽ‹æ°‘ï¼‰"
    const editorMatch = filename.match(/ï¼ˆä¸»ç¼–ï¼š([^ï¼‰]+)ï¼‰/);
    if (editorMatch) return `ä¸»ç¼–:${editorMatch[1]}`;
    
    return 'unknown';
  }

  /**
   * Flatten nested metadata object to CSV-friendly format
   */
  flattenMetadata(metadata) {
    const flattened = {};
    
    for (const [key, value] of Object.entries(metadata || {})) {
      if (value === null || value === undefined) {
        flattened[key] = '';
      } else if (typeof value === 'object' && !Array.isArray(value)) {
        // Flatten nested objects with dot notation
        for (const [subKey, subValue] of Object.entries(value)) {
          flattened[`${key}_${subKey}`] = String(subValue || '');
        }
      } else if (Array.isArray(value)) {
        flattened[key] = value.join('|');
      } else {
        flattened[key] = String(value);
      }
    }
    
    return flattened;
  }

  /**
   * Read and process a single JSON file
   */
  async processJsonFile(filename) {
    try {
      const filePath = path.join(this.inputDir, filename);
      const content = await fs.readFile(filePath, 'utf8');
      const data = JSON.parse(content);
      
      // Handle enhanced format - data is array of chunks directly
      let chunks;
      if (Array.isArray(data)) {
        chunks = data;
      } else if (data.chunks && Array.isArray(data.chunks)) {
        // Legacy format support
        chunks = data.chunks;
      } else {
        console.warn(`âš ï¸ Invalid format in ${filename} - expected array or object with chunks property`);
        return null;
      }

      if (chunks.length === 0) {
        console.warn(`âš ï¸ No chunks found in ${filename}`);
        return null;
      }

      console.log(`ðŸ“– Processing ${filename}: ${chunks.length} chunks`);
      
      return {
        filename,
        bookName: this.extractBookName(filename),
        chunks: chunks.filter(chunk => chunk.content && chunk.content.trim().length > 0)
      };
      
    } catch (error) {
      console.error(`âŒ Failed to process ${filename}:`, error.message);
      return null;
    }
  }

  /**
   * Create CSV file for a book
   */
  async createBookCSV(bookData) {
    try {
      const { filename, bookName, chunks } = bookData;
      console.log(`ðŸ“ Creating CSV for: ${bookName}...`);
      
      const csvFilePath = path.join(this.outputDir, `${bookName}.csv`);
      
      // Prepare data for CSV
      const csvData = [];
      const allMetadataKeys = new Set();
      
      // First pass: collect all metadata keys to ensure consistent CSV headers
      for (const chunk of chunks) {
        const educationalMeta = this.parseEducationalMetadata(filename, chunk.metadata);
        const flattenedMetadata = this.flattenMetadata(educationalMeta);
        Object.keys(flattenedMetadata).forEach(key => allMetadataKeys.add(key));
        
        // Also collect semantic features keys
        if (chunk.semanticFeatures) {
          const semanticFlat = this.flattenMetadata({ semanticFeatures: chunk.semanticFeatures });
          Object.keys(semanticFlat).forEach(key => allMetadataKeys.add(key));
        }
      }
      
      // Sort metadata keys for consistent column order
      const sortedMetadataKeys = Array.from(allMetadataKeys).sort();
      
      // Second pass: create CSV rows
      for (let i = 0; i < chunks.length; i++) {
        const chunk = chunks[i];
        
        // Parse educational metadata
        const educationalMeta = this.parseEducationalMetadata(filename, chunk.metadata);
        const flattenedMetadata = this.flattenMetadata(educationalMeta);
        
        // Add semantic features
        if (chunk.semanticFeatures) {
          const semanticFlat = this.flattenMetadata({ semanticFeatures: chunk.semanticFeatures });
          Object.assign(flattenedMetadata, semanticFlat);
        }
        
        const row = {
          chunk_id: `${bookName}_chunk_${i}`,
          chunk_index: i,
          content: chunk.content.replace(/\n/g, '\\n').replace(/\r/g, '\\r'), // Escape newlines
          content_length: chunk.content.length,
          quality_score: chunk.qualityScore || 0,
          reliability: chunk.reliability || 'unknown',
          source_type: chunk.sourceType || 'text-extracted',
          original_content: chunk.originalContent ? chunk.originalContent.replace(/\n/g, '\\n').replace(/\r/g, '\\r') : '',
          enhancement_version: chunk.metadata?.enhancementVersion || 'unknown'
        };
        
        // Add all metadata fields
        for (const key of sortedMetadataKeys) {
          row[key] = flattenedMetadata[key] || '';
        }
        
        csvData.push(row);
      }
      
      // Define CSV headers
      const headers = [
        { id: 'chunk_id', title: 'Chunk_ID' },
        { id: 'chunk_index', title: 'Chunk_Index' },
        { id: 'content', title: 'Content' },
        { id: 'content_length', title: 'Content_Length' },
        { id: 'quality_score', title: 'Quality_Score' },
        { id: 'reliability', title: 'Reliability' },
        { id: 'source_type', title: 'Source_Type' },
        { id: 'original_content', title: 'Original_Content' },
        { id: 'enhancement_version', title: 'Enhancement_Version' }
      ];
      
      // Add metadata headers
      for (const key of sortedMetadataKeys) {
        headers.push({ id: key, title: key });
      }
      
      // Create CSV writer
      const csvWriter = createCsvWriter({
        path: csvFilePath,
        header: headers,
        encoding: 'utf8'
      });
      
      // Write CSV file
      await csvWriter.writeRecords(csvData);
      
      const fileStats = await fs.stat(csvFilePath);
      console.log(`âœ… Created CSV: ${csvFilePath}`);
      console.log(`   - Records: ${csvData.length}`);
      console.log(`   - Columns: ${headers.length}`);
      console.log(`   - File size: ${fileStats.size} bytes`);
      
      return {
        bookName,
        filePath: csvFilePath,
        recordCount: csvData.length,
        columnCount: headers.length,
        fileSize: fileStats.size
      };
      
    } catch (error) {
      console.error(`âŒ Failed to create CSV for ${bookData.bookName}:`, error.message);
      throw error;
    }
  }

  /**
   * Create summary report of the export
   */
  async createSummaryReport(exportResults) {
    const summaryPath = path.join(this.outputDir, 'export_summary.json');
    
    const summary = {
      exportDate: new Date().toISOString(),
      exportType: 'JSON_to_CSV_Direct',
      totalBooks: exportResults.length,
      totalRecords: exportResults.reduce((sum, result) => sum + result.recordCount, 0),
      totalFileSize: exportResults.reduce((sum, result) => sum + result.fileSize, 0),
      books: exportResults.map(result => ({
        bookName: result.bookName,
        fileName: path.basename(result.filePath),
        recordCount: result.recordCount,
        columnCount: result.columnCount,
        fileSizeBytes: result.fileSize
      }))
    };
    
    await fs.writeFile(summaryPath, JSON.stringify(summary, null, 2), 'utf8');
    console.log(`ðŸ“‹ Export summary saved: ${summaryPath}`);
    
    return summary;
  }

  /**
   * Main export function
   */
  async exportToCSV() {
    console.log('ðŸš€ Starting JSON to CSV export...');
    
    try {
      // Initialize
      await this.initialize();
      
      // Get all JSON files
      const files = await fs.readdir(this.inputDir);
      const jsonFiles = files.filter(file => file.endsWith('.json'));
      
      console.log(`ðŸ“š Found ${jsonFiles.length} JSON files to process`);
      
      if (jsonFiles.length === 0) {
        throw new Error('No JSON files found in the input directory');
      }
      
      // Group files by book name to avoid duplicates
      const bookGroups = new Map();
      
      for (const filename of jsonFiles) {
        const bookName = this.extractBookName(filename);
        if (!bookGroups.has(bookName)) {
          bookGroups.set(bookName, []);
        }
        bookGroups.get(bookName).push(filename);
      }
      
      console.log(`ðŸ“– Organized into ${bookGroups.size} unique books`);
      
      // Process each book group
      const exportResults = [];
      let processedBooks = 0;
      
      for (const [bookName, filenames] of bookGroups) {
        try {
          console.log(`\nðŸ“š Processing book group: ${bookName} (${filenames.length} files)`);
          
          // For now, process the first file in each book group
          // In the future, we could merge multiple files for the same book
          const primaryFile = filenames[0];
          if (filenames.length > 1) {
            console.log(`   ðŸ“ Note: Using primary file ${primaryFile}, others: ${filenames.slice(1).join(', ')}`);
          }
          
          const bookData = await this.processJsonFile(primaryFile);
          if (!bookData) {
            console.log(`   âš ï¸ Skipped ${bookName}: No valid data`);
            continue;
          }
          
          const result = await this.createBookCSV(bookData);
          exportResults.push(result);
          processedBooks++;
          
          console.log(`   âœ… Completed ${bookName}: ${result.recordCount} records`);
          console.log(`ðŸ“ˆ Progress: ${processedBooks}/${bookGroups.size} books processed`);
          
        } catch (error) {
          console.error(`âš ï¸ Skipped book ${bookName}:`, error.message);
        }
      }
      
      // Create summary report
      const summary = await this.createSummaryReport(exportResults);
      
      console.log('\nðŸŽ‰ Export completed successfully!');
      console.log(`ðŸ“ Output directory: ${this.outputDir}`);
      console.log(`ðŸ“š Books exported: ${exportResults.length}/${bookGroups.size}`);
      console.log(`ðŸ“„ Total records: ${summary.totalRecords}`);
      console.log(`ðŸ“Š Total file size: ${(summary.totalFileSize / 1024 / 1024).toFixed(2)} MB`);
      console.log(`ðŸ“‹ Summary report: ${path.join(this.outputDir, 'export_summary.json')}`);
      
      return {
        success: true,
        outputDir: this.outputDir,
        summary
      };
      
    } catch (error) {
      console.error('âŒ Export failed:', error.message);
      throw error;
    }
  }
}

// CLI execution
if (require.main === module) {
  const exporter = new JSONToCSVExporter();
  
  exporter.exportToCSV()
    .then((result) => {
      console.log('\nâœ¨ Export process completed successfully');
      process.exit(0);
    })
    .catch((error) => {
      console.error('\nðŸ’¥ Export process failed:', error.message);
      console.error(error.stack);
      process.exit(1);
    });
}

module.exports = JSONToCSVExporter;