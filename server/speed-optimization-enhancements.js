/**
 * AI Response Speed Optimization Enhancements
 * Implements comprehensive solutions for slow AI response times
 */

const logger = require("./utils/logger");

class SpeedOptimizationEnhancements {
  constructor(aiService) {
    this.aiService = aiService;
    this.responseTimeTargets = {
      lesson_plan: 8000,   // 8 seconds max
      exercises: 6000,     // 6 seconds max
      quick_answer: 3000,  // 3 seconds max
      analysis: 2000,      // 2 seconds max
    };
  }

  /**
   * Optimize model parameters for faster responses
   */
  getSpeedOptimizedParams(requestType, originalParams) {
    const speedOptimizations = {
      lesson_plan: {
        // Reduce token count for faster generation
        maxTokens: Math.min(originalParams.maxTokens || 2000, 1500),
        // Increase temperature slightly for less overthinking
        temperature: Math.min((originalParams.temperature || 0.7) + 0.1, 1.0),
        // Use more focused generation
        topP: Math.max((originalParams.topP || 0.8) - 0.1, 0.7),
        // Enable faster model selection
        model: "qwen-turbo", // Faster than qwen-plus
      },
      exercises: {
        maxTokens: Math.min(originalParams.maxTokens || 1500, 1000),
        temperature: 0.6, // More deterministic = faster
        topP: 0.8,
        model: "qwen-turbo",
      },
      quick_answer: {
        maxTokens: 300, // Very limited for speed
        temperature: 0.3, // Fast and deterministic
        topP: 0.7,
        model: "qwen-turbo",
      },
      analysis: {
        maxTokens: 200,
        temperature: 0.1, // Minimal variation for speed
        topP: 0.6,
        model: "qwen-turbo",
      }
    };

    return {
      ...originalParams,
      ...speedOptimizations[requestType],
    };
  }

  /**
   * Implement progressive streaming for better perceived speed
   */
  async generateWithProgressiveStreaming(systemPrompt, userPrompt, res, requestType) {
    const startTime = Date.now();
    
    try {
      // Send immediate response header
      res.setHeader("Content-Type", "text/plain; charset=utf-8");
      res.setHeader("Cache-Control", "no-cache");
      res.setHeader("Connection", "keep-alive");

      // Send early loading indicator
      res.write("ğŸš€ AIæ­£åœ¨æ€è€ƒä¸­...\n\n");

      // Get optimized parameters
      const optimizedParams = this.getSpeedOptimizedParams(requestType, {
        model: this.aiService.model,
        maxTokens: this.aiService.maxTokens,
        temperature: this.aiService.temperature,
        topP: this.aiService.topP,
      });

      logger.info("Speed optimization applied", {
        requestType,
        originalModel: this.aiService.model,
        optimizedModel: optimizedParams.model,
        tokenReduction: this.aiService.maxTokens - optimizedParams.maxTokens,
      });

      // Create streaming request with timeout
      const streamPromise = this.createTimeoutWrappedStream(
        systemPrompt,
        userPrompt,
        optimizedParams,
        this.responseTimeTargets[requestType] || 10000
      );

      let contentStarted = false;
      let totalContent = "";

      for await (const chunk of streamPromise) {
        if (chunk.choices && chunk.choices[0] && chunk.choices[0].delta) {
          const content = chunk.choices[0].delta.content;
          if (content) {
            // Remove loading indicator when real content starts
            if (!contentStarted) {
              res.write("\rğŸ¯ ç”Ÿæˆå¼€å§‹ï¼š\n\n");
              contentStarted = true;
            }
            
            totalContent += content;
            res.write(content);
          }
        }

        // Handle usage information
        if (chunk.usage) {
          const responseTime = Date.now() - startTime;
          logger.info("Streaming completed", {
            requestType,
            responseTime,
            tokenCount: chunk.usage.completion_tokens,
            speed: `${(chunk.usage.completion_tokens / (responseTime / 1000)).toFixed(2)} tokens/sec`,
          });
        }
      }

      res.end();
      
      const finalResponseTime = Date.now() - startTime;
      this.logPerformanceMetrics(requestType, finalResponseTime, totalContent.length);

    } catch (error) {
      const responseTime = Date.now() - startTime;
      logger.error("Speed-optimized generation failed", {
        requestType,
        responseTime,
        error: error.message,
      });

      if (!res.headersSent) {
        res.status(500);
      }
      res.write(`\nâŒ ç”Ÿæˆå¤±è´¥: ${error.message}`);
      res.end();
    }
  }

  /**
   * Create timeout-wrapped stream to prevent hanging
   */
  async createTimeoutWrappedStream(systemPrompt, userPrompt, params, timeoutMs) {
    const timeoutPromise = new Promise((_, reject) => {
      setTimeout(() => {
        reject(new Error(`è¯·æ±‚è¶…æ—¶ (${timeoutMs}ms) - è¯·é‡è¯•æˆ–ç®€åŒ–è¯·æ±‚`));
      }, timeoutMs);
    });

    const streamPromise = this.aiService.openai.chat.completions.create({
      model: params.model,
      messages: [
        { role: "system", content: systemPrompt },
        { role: "user", content: userPrompt },
      ],
      max_tokens: params.maxTokens,
      temperature: params.temperature,
      top_p: params.topP,
      stream: true,
    });

    return Promise.race([streamPromise, timeoutPromise]);
  }

  /**
   * Implement response chunking for large content
   */
  async generateWithChunking(systemPrompt, userPrompt, res, requestType) {
    // For very large responses, break into smaller chunks
    const chunkSize = 500; // tokens per chunk
    const maxChunks = 4;

    const basePrompt = systemPrompt;
    const chunkPrompts = this.createChunkPrompts(userPrompt, maxChunks);

    res.setHeader("Content-Type", "text/plain; charset=utf-8");
    res.write("ğŸ“ åˆ†æ®µç”Ÿæˆä¸­...\n\n");

    for (let i = 0; i < chunkPrompts.length; i++) {
      try {
        res.write(`## ç¬¬${i + 1}éƒ¨åˆ†\n\n`);
        
        const chunkParams = this.getSpeedOptimizedParams(requestType, {
          maxTokens: chunkSize,
          temperature: this.aiService.temperature,
          topP: this.aiService.topP,
        });

        const completion = await this.aiService.openai.chat.completions.create({
          model: chunkParams.model,
          messages: [
            { role: "system", content: basePrompt },
            { role: "user", content: chunkPrompts[i] },
          ],
          max_tokens: chunkParams.maxTokens,
          temperature: chunkParams.temperature,
          top_p: chunkParams.topP,
        });

        const content = completion.choices[0].message.content;
        res.write(content + "\n\n");

      } catch (error) {
        logger.error("Chunk generation failed", {
          chunkIndex: i,
          error: error.message,
        });
        res.write(`âŒ ç¬¬${i + 1}éƒ¨åˆ†ç”Ÿæˆå¤±è´¥\n\n`);
      }
    }

    res.end();
  }

  /**
   * Create smaller prompts for chunked generation
   */
  createChunkPrompts(originalPrompt, maxChunks) {
    // Split complex requests into focused chunks
    const chunkTemplates = {
      lesson_plan: [
        "è¯·ç”Ÿæˆæ•™å­¦ç›®æ ‡å’Œé‡ç‚¹éš¾ç‚¹éƒ¨åˆ†",
        "è¯·ç”Ÿæˆæ•™å­¦è¿‡ç¨‹å’Œæ–¹æ³•éƒ¨åˆ†", 
        "è¯·ç”Ÿæˆè¯¾å ‚æ´»åŠ¨å’Œç»ƒä¹ éƒ¨åˆ†",
        "è¯·ç”Ÿæˆä½œä¸šå®‰æ’å’Œæ•™å­¦åæ€éƒ¨åˆ†",
      ],
      exercises: [
        "è¯·ç”Ÿæˆé€‰æ‹©é¢˜éƒ¨åˆ†",
        "è¯·ç”Ÿæˆå¡«ç©ºé¢˜å’Œåˆ¤æ–­é¢˜éƒ¨åˆ†",
        "è¯·ç”Ÿæˆè®¡ç®—é¢˜å’Œåº”ç”¨é¢˜éƒ¨åˆ†",
        "è¯·ç”Ÿæˆç­”æ¡ˆè§£æéƒ¨åˆ†",
      ],
    };

    // Use template if available, otherwise split by content
    if (chunkTemplates.lesson_plan) {
      return chunkTemplates.lesson_plan.map(template => 
        `${originalPrompt}\n\nå…·ä½“è¦æ±‚ï¼š${template}`
      );
    }

    // Fallback: split original prompt
    const words = originalPrompt.split(" ");
    const chunkSize = Math.ceil(words.length / maxChunks);
    const chunks = [];

    for (let i = 0; i < maxChunks && i * chunkSize < words.length; i++) {
      const chunkWords = words.slice(i * chunkSize, (i + 1) * chunkSize);
      chunks.push(chunkWords.join(" "));
    }

    return chunks;
  }

  /**
   * Implement smart retries with fallback strategies
   */
  async generateWithFallback(systemPrompt, userPrompt, res, requestType) {
    const strategies = [
      // Strategy 1: Full speed optimization
      () => this.generateWithProgressiveStreaming(systemPrompt, userPrompt, res, requestType),
      
      // Strategy 2: Reduced complexity
      () => this.generateWithProgressiveStreaming(
        this.simplifyPrompt(systemPrompt), 
        this.simplifyPrompt(userPrompt), 
        res, 
        requestType
      ),
      
      // Strategy 3: Chunked generation
      () => this.generateWithChunking(systemPrompt, userPrompt, res, requestType),
      
      // Strategy 4: Basic fallback
      () => this.generateBasicResponse(userPrompt, res),
    ];

    for (let i = 0; i < strategies.length; i++) {
      try {
        logger.info(`Attempting generation strategy ${i + 1}`, { requestType });
        await strategies[i]();
        return; // Success, exit
      } catch (error) {
        logger.warn(`Generation strategy ${i + 1} failed`, {
          requestType,
          error: error.message,
          retryAvailable: i < strategies.length - 1,
        });
        
        if (i === strategies.length - 1) {
          // Last strategy failed
          if (!res.headersSent) {
            res.status(500);
          }
          res.write(`\nâŒ æ‰€æœ‰ç”Ÿæˆç­–ç•¥å‡å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•`);
          res.end();
        }
      }
    }
  }

  /**
   * Simplify prompts for faster processing
   */
  simplifyPrompt(prompt) {
    return prompt
      .replace(/è¯¦ç»†|å…·ä½“|æ·±å…¥|å…¨é¢/g, "ç®€è¦")
      .replace(/å®Œæ•´çš„|å®Œå…¨çš„/g, "åŸºæœ¬çš„")
      .replace(/è¯·è¯¦ç»†è¯´æ˜/g, "è¯·ç®€è¦è¯´æ˜")
      .substring(0, Math.min(prompt.length, 500)); // Limit prompt length
  }

  /**
   * Generate basic fallback response
   */
  async generateBasicResponse(userPrompt, res) {
    const basicResponse = `
# åŸºç¡€æ•™å­¦å†…å®¹

## æ•™å­¦ç›®æ ‡
- æ ¹æ®è¯¾ç¨‹è¦æ±‚åˆ¶å®šç›¸åº”çš„å­¦ä¹ ç›®æ ‡
- åŸ¹å…»å­¦ç”Ÿçš„åŸºç¡€çŸ¥è¯†å’ŒæŠ€èƒ½

## æ•™å­¦é‡ç‚¹
- æ ¸å¿ƒæ¦‚å¿µçš„ç†è§£å’ŒæŒæ¡
- åŸºæœ¬æ–¹æ³•çš„å­¦ä¹ å’Œåº”ç”¨

## æ•™å­¦è¿‡ç¨‹
1. å¯¼å…¥æ–°è¯¾ (5åˆ†é’Ÿ)
2. æ–°è¯¾è®²è§£ (25åˆ†é’Ÿ)  
3. è¯¾å ‚ç»ƒä¹  (10åˆ†é’Ÿ)
4. æ€»ç»“ä½œä¸š (5åˆ†é’Ÿ)

## ä½œä¸šå®‰æ’
- å®Œæˆç›¸å…³ç»ƒä¹ é¢˜
- é¢„ä¹ ä¸‹èŠ‚è¯¾å†…å®¹

---
âš¡ è¿™æ˜¯ä¸€ä¸ªåŸºç¡€æ¨¡æ¿ï¼Œå»ºè®®æ ¹æ®å…·ä½“éœ€æ±‚è¿›è¡Œè°ƒæ•´
`;

    res.setHeader("Content-Type", "text/plain; charset=utf-8");
    res.write(basicResponse);
    res.end();
  }

  /**
   * Log performance metrics
   */
  logPerformanceMetrics(requestType, responseTime, contentLength) {
    const target = this.responseTimeTargets[requestType] || 10000;
    const isWithinTarget = responseTime <= target;
    const speed = contentLength / (responseTime / 1000);

    logger.info("Performance metrics", {
      requestType,
      responseTime,
      target,
      withinTarget: isWithinTarget,
      contentLength,
      generationSpeed: `${speed.toFixed(2)} chars/sec`,
      performance: isWithinTarget ? "âœ… GOOD" : "âš ï¸ SLOW",
    });

    // Log recommendations if slow
    if (!isWithinTarget) {
      const recommendations = this.getSpeedRecommendations(requestType, responseTime);
      logger.warn("Speed optimization recommendations", {
        requestType,
        recommendations,
      });
    }
  }

  /**
   * Get speed optimization recommendations
   */
  getSpeedRecommendations(requestType, responseTime) {
    const recommendations = [];
    
    if (responseTime > 15000) {
      recommendations.push("å“åº”æ—¶é—´è¿‡é•¿ï¼Œå»ºè®®ç®€åŒ–è¯·æ±‚æˆ–åˆ†æ®µç”Ÿæˆ");
    }
    
    if (responseTime > 10000) {
      recommendations.push("è€ƒè™‘ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹å¦‚qwen-turbo");
      recommendations.push("å‡å°‘æœ€å¤§tokenæ•°é‡");
    }
    
    if (responseTime > 8000) {
      recommendations.push("å¯ç”¨ç¼“å­˜æœºåˆ¶");
      recommendations.push("ä¼˜åŒ–æç¤ºè¯é•¿åº¦");
    }

    return recommendations;
  }

  /**
   * Get speed optimization report
   */
  getSpeedReport() {
    return {
      responseTimeTargets: this.responseTimeTargets,
      optimizations: {
        progressiveStreaming: "å®æ—¶æµå¼è¾“å‡ºæå‡æ„ŸçŸ¥é€Ÿåº¦",
        modelOptimization: "ä½¿ç”¨qwen-turboæ›¿ä»£qwen-plus",
        tokenReduction: "æ™ºèƒ½å‡å°‘tokenæ•°é‡",
        timeoutProtection: "é˜²æ­¢è¯·æ±‚è¶…æ—¶æŒ‚èµ·",
        chunkingSupport: "å¤§å†…å®¹åˆ†æ®µç”Ÿæˆ",
        fallbackStrategies: "å¤šé‡å›é€€ç­–ç•¥",
        caching: "å“åº”ç¼“å­˜æœºåˆ¶",
      },
      recommendations: [
        "ç›‘æ§å“åº”æ—¶é—´å¹¶åŠæ—¶è°ƒæ•´å‚æ•°",
        "æ ¹æ®è¯·æ±‚ç±»å‹é€‰æ‹©ä¸åŒä¼˜åŒ–ç­–ç•¥",
        "å®æ–½ç”¨æˆ·åé¦ˆæ”¶é›†æœºåˆ¶",
        "å®šæœŸè¯„ä¼°å’Œæ›´æ–°ä¼˜åŒ–å‚æ•°",
      ],
    };
  }
}

module.exports = SpeedOptimizationEnhancements;