/**
 * Edge Case and Stress Testing Suite
 * Tests system behavior outside the happy path
 * Based on 2024 best practices for AI system stress testing
 */

const AIService = require("../ai-service");
const VectorStore = require("../rag/services/vector-store");

// Mock dependencies
jest.mock("../rag/services/vector-store");
jest.mock("openai");
jest.mock("../utils/logger");

describe("Edge Case and Stress Testing", () => {
  let aiService;
  let vectorStore;
  let mockResponse;

  beforeEach(() => {
    process.env.DASHSCOPE_API_KEY = "test-api-key";
    process.env.AI_ENABLED = "true";

    mockResponse = {
      setHeader: jest.fn(),
      write: jest.fn(),
      end: jest.fn(),
      status: jest.fn().mockReturnThis(),
      headersSent: false,
    };

    vectorStore = new VectorStore();
    aiService = new AIService();
  });

  afterEach(() => {
    delete process.env.DASHSCOPE_API_KEY;
    delete process.env.AI_ENABLED;
    jest.clearAllMocks();
  });

  describe("Input Edge Cases", () => {
    test("should handle extremely long input gracefully", async () => {
      const extremelyLongTopic = "æ•°å­¦".repeat(1000); // 2000 characters
      const extremelyLongRequirements = "è¯¦ç»†è®²è§£".repeat(500); // 2000 characters

      vectorStore.getRelevantContext.mockResolvedValue({
        context: "åŸºç¡€æ•°å­¦å†…å®¹",
        sources: ["basic.pdf"],
        totalResults: 1,
        usedResults: 1,
      });

      const mockStream = {
        [Symbol.asyncIterator]: async function* () {
          yield {
            choices: [{
              delta: { content: "å¤„ç†è¶…é•¿è¾“å…¥çš„æ•™æ¡ˆå†…å®¹..." }
            }]
          };
        },
      };

      aiService.openai = {
        chat: {
          completions: {
            create: jest.fn().mockResolvedValue(mockStream),
          },
        },
      };

      await expect(
        aiService.generateLessonPlanStream(
          "æ•°å­¦",
          "ä¸‰å¹´çº§",
          extremelyLongTopic,
          extremelyLongRequirements,
          mockResponse
        )
      ).resolves.not.toThrow();

      expect(mockResponse.write).toHaveBeenCalled();
      expect(mockResponse.end).toHaveBeenCalled();
    });

    test("should handle special characters and emojis", async () => {
      const specialTopic = "æ•°å­¦ğŸ”¢â•â–âœ–ï¸â—ğŸ“ğŸ“ğŸ“ŠğŸ“ˆ";
      const emojiRequirements = "ç”¨ğŸ¯é‡ç‚¹è®²è§£ğŸ“šï¼ŒåŒ…å«ğŸ¤”æ€è€ƒé¢˜â“";

      vectorStore.getRelevantContext.mockResolvedValue({
        context: "æ•°å­¦æ•™å­¦å†…å®¹",
        sources: ["math.pdf"],
        totalResults: 1,
        usedResults: 1,
      });

      const mockStream = {
        [Symbol.asyncIterator]: async function* () {
          yield {
            choices: [{
              delta: { content: "# æ•°å­¦æ•™æ¡ˆ\nåŒ…å«ç‰¹æ®Šå­—ç¬¦çš„å†…å®¹..." }
            }]
          };
        },
      };

      aiService.openai = {
        chat: {
          completions: {
            create: jest.fn().mockResolvedValue(mockStream),
          },
        },
      };

      await aiService.generateLessonPlanStream(
        "æ•°å­¦",
        "ä¸‰å¹´çº§",
        specialTopic,
        emojiRequirements,
        mockResponse
      );

      const writtenContent = mockResponse.write.mock.calls
        .map(call => call[0])
        .join("");

      // Should handle special characters without breaking
      expect(writtenContent).toBeDefined();
      expect(writtenContent.length).toBeGreaterThan(0);
    });

    test("should handle malformed Unicode input", async () => {
      const malformedInput = "æ•°å­¦\uD800æ•™å­¦"; // Unpaired surrogate
      const mixedEncodingInput = "æ•°å­¦mathæ•°å­¦";

      vectorStore.getRelevantContext.mockResolvedValue({
        context: "æ•™å­¦å†…å®¹",
        sources: ["guide.pdf"],
        totalResults: 1,
        usedResults: 1,
      });

      const mockStream = {
        [Symbol.asyncIterator]: async function* () {
          yield {
            choices: [{
              delta: { content: "å¤„ç†ç‰¹æ®Šç¼–ç çš„æ•™æ¡ˆ..." }
            }]
          };
        },
      };

      aiService.openai = {
        chat: {
          completions: {
            create: jest.fn().mockResolvedValue(mockStream),
          },
        },
      };

      await expect(
        aiService.generateLessonPlanStream(
          "æ•°å­¦",
          "ä¸‰å¹´çº§",
          malformedInput,
          mixedEncodingInput,
          mockResponse
        )
      ).resolves.not.toThrow();
    });

    test("should handle empty and null inputs", async () => {
      const testCases = [
        { subject: "", grade: "", topic: "", requirements: "" },
        { subject: null, grade: null, topic: null, requirements: null },
        { subject: undefined, grade: undefined, topic: undefined, requirements: undefined },
        { subject: "   ", grade: "   ", topic: "   ", requirements: "   " }, // Whitespace only
      ];

      for (const testCase of testCases) {
        vectorStore.getRelevantContext.mockResolvedValue({
          context: "",
          sources: [],
          totalResults: 0,
          usedResults: 0,
        });

        const mockStream = {
          [Symbol.asyncIterator]: async function* () {
            yield {
              choices: [{
                delta: { content: "é»˜è®¤æ•™æ¡ˆå†…å®¹" }
              }]
            };
          },
        };

        aiService.openai = {
          chat: {
            completions: {
              create: jest.fn().mockResolvedValue(mockStream),
            },
          },
        };

        // Should handle gracefully without throwing
        await expect(
          aiService.generateLessonPlanStream(
            testCase.subject,
            testCase.grade,
            testCase.topic,
            testCase.requirements,
            mockResponse
          )
        ).resolves.not.toThrow();
      }
    });
  });

  describe("Network and API Edge Cases", () => {
    test("should handle API timeout gracefully", async () => {
      const timeoutError = new Error("Request timeout");
      timeoutError.code = "ECONNABORTED";

      aiService.openai = {
        chat: {
          completions: {
            create: jest.fn().mockRejectedValue(timeoutError),
          },
        },
      };

      vectorStore.getRelevantContext.mockResolvedValue({
        context: "æ•™å­¦å†…å®¹",
        sources: ["guide.pdf"],
        totalResults: 1,
        usedResults: 1,
      });

      await aiService.generateLessonPlanStream(
        "æ•°å­¦",
        "ä¸‰å¹´çº§",
        "å°æ•°åŠ æ³•",
        null,
        mockResponse
      );

      expect(mockResponse.status).toHaveBeenCalledWith(500);
      expect(mockResponse.write).toHaveBeenCalledWith(
        expect.stringContaining("ç½‘ç»œè¶…æ—¶")
      );
    });

    test("should handle API rate limiting", async () => {
      const rateLimitError = new Error("Rate limit exceeded");
      rateLimitError.status = 429;

      aiService.openai = {
        chat: {
          completions: {
            create: jest.fn().mockRejectedValueOnce(rateLimitError),
          },
        },
      };

      vectorStore.getRelevantContext.mockResolvedValue({
        context: "æ•™å­¦å†…å®¹",
        sources: ["guide.pdf"],
        totalResults: 1,
        usedResults: 1,
      });

      await aiService.generateLessonPlanStream(
        "æ•°å­¦",
        "ä¸‰å¹´çº§",
        "å°æ•°åŠ æ³•",
        null,
        mockResponse
      );

      expect(mockResponse.status).toHaveBeenCalledWith(500);
      expect(mockResponse.write).toHaveBeenCalledWith(
        expect.stringContaining("AIæœåŠ¡é”™è¯¯")
      );
    });

    test("should handle corrupted streaming response", async () => {
      const corruptedStream = {
        [Symbol.asyncIterator]: async function* () {
          yield { choices: [{ delta: { content: "æ­£å¸¸å†…å®¹" } }] };
          yield { choices: [{ delta: { content: null } }] }; // Corrupted chunk
          yield { invalid: "data" }; // Invalid structure
          yield { choices: [{ delta: { content: "æ¢å¤æ­£å¸¸" } }] };
        },
      };

      aiService.openai = {
        chat: {
          completions: {
            create: jest.fn().mockResolvedValue(corruptedStream),
          },
        },
      };

      vectorStore.getRelevantContext.mockResolvedValue({
        context: "æ•™å­¦å†…å®¹",
        sources: ["guide.pdf"],
        totalResults: 1,
        usedResults: 1,
      });

      await aiService.generateLessonPlanStream(
        "æ•°å­¦",
        "ä¸‰å¹´çº§",
        "å°æ•°åŠ æ³•",
        null,
        mockResponse
      );

      // Should handle corrupted chunks gracefully
      const writtenContent = mockResponse.write.mock.calls
        .map(call => call[0])
        .join("");

      expect(writtenContent).toContain("æ­£å¸¸å†…å®¹");
      expect(writtenContent).toContain("æ¢å¤æ­£å¸¸");
      expect(mockResponse.end).toHaveBeenCalled();
    });
  });

  describe("Vector Database Edge Cases", () => {
    test("should handle vector database unavailable", async () => {
      vectorStore.getRelevantContext.mockRejectedValue(
        new Error("ChromaDB connection failed")
      );

      const mockStream = {
        [Symbol.asyncIterator]: async function* () {
          yield {
            choices: [{
              delta: { content: "æ— RAGå¢å¼ºçš„æ•™æ¡ˆå†…å®¹" }
            }]
          };
        },
      };

      aiService.openai = {
        chat: {
          completions: {
            create: jest.fn().mockResolvedValue(mockStream),
          },
        },
      };

      // Should continue without RAG enhancement
      await aiService.generateLessonPlanStream(
        "æ•°å­¦",
        "ä¸‰å¹´çº§",
        "å°æ•°åŠ æ³•",
        null,
        mockResponse
      );

      expect(mockResponse.write).toHaveBeenCalledWith(
        expect.stringContaining("æ— RAGå¢å¼º")
      );
      expect(mockResponse.end).toHaveBeenCalled();
    });

    test("should handle empty vector database", async () => {
      vectorStore.getRelevantContext.mockResolvedValue({
        context: "",
        sources: [],
        totalResults: 0,
        usedResults: 0,
      });

      const mockStream = {
        [Symbol.asyncIterator]: async function* () {
          yield {
            choices: [{
              delta: { content: "åŸºäºé€šç”¨çŸ¥è¯†çš„æ•™æ¡ˆ" }
            }]
          };
        },
      };

      aiService.openai = {
        chat: {
          completions: {
            create: jest.fn().mockResolvedValue(mockStream),
          },
        },
      };

      await aiService.generateLessonPlanStream(
        "éå¸¸ç½•è§çš„ä¸»é¢˜",
        "ç‰¹æ®Šå¹´çº§",
        "ä¸å­˜åœ¨çš„è¯¾é¢˜",
        null,
        mockResponse
      );

      expect(mockResponse.write).toHaveBeenCalledWith(
        expect.stringContaining("åŸºäºé€šç”¨çŸ¥è¯†")
      );
    });
  });

  describe("Stress Testing", () => {
    test("should handle high concurrent request load", async () => {
      const concurrentRequests = 10;
      const promises = [];

      vectorStore.getRelevantContext.mockResolvedValue({
        context: "æ•™å­¦å†…å®¹",
        sources: ["guide.pdf"],
        totalResults: 1,
        usedResults: 1,
      });

      const mockStream = {
        [Symbol.asyncIterator]: async function* () {
          yield {
            choices: [{
              delta: { content: "å¹¶å‘å¤„ç†çš„æ•™æ¡ˆå†…å®¹" }
            }]
          };
        },
      };

      aiService.openai = {
        chat: {
          completions: {
            create: jest.fn().mockResolvedValue(mockStream),
          },
        },
      };

      // Create concurrent requests
      for (let i = 0; i < concurrentRequests; i++) {
        const mockRes = {
          setHeader: jest.fn(),
          write: jest.fn(),
          end: jest.fn(),
          status: jest.fn().mockReturnThis(),
        };

        promises.push(
          aiService.generateLessonPlanStream(
            "æ•°å­¦",
            "ä¸‰å¹´çº§",
            `ä¸»é¢˜${i}`,
            null,
            mockRes
          )
        );
      }

      // All requests should complete successfully
      await expect(Promise.all(promises)).resolves.not.toThrow();

      // Verify all requests were processed
      expect(aiService.openai.chat.completions.create).toHaveBeenCalledTimes(
        concurrentRequests
      );
    });

    test("should handle memory pressure scenarios", async () => {
      const largeContextSize = 10000; // Very large context
      const largeContext = "è¯¦ç»†æ•™å­¦å†…å®¹".repeat(largeContextSize / 6);

      vectorStore.getRelevantContext.mockResolvedValue({
        context: largeContext,
        sources: Array.from({ length: 100 }, (_, i) => `file${i}.pdf`),
        totalResults: 100,
        usedResults: 50,
      });

      const largeContentStream = {
        [Symbol.asyncIterator]: async function* () {
          // Generate a large amount of content
          for (let i = 0; i < 1000; i++) {
            yield {
              choices: [{
                delta: { content: `æ•™æ¡ˆå†…å®¹ç‰‡æ®µ${i}ã€‚`.repeat(10) }
              }]
            };
          }
        },
      };

      aiService.openai = {
        chat: {
          completions: {
            create: jest.fn().mockResolvedValue(largeContentStream),
          },
        },
      };

      const startMemory = process.memoryUsage().heapUsed;

      await aiService.generateLessonPlanStream(
        "æ•°å­¦",
        "ä¸‰å¹´çº§",
        "å¤æ‚çš„æ•°å­¦ä¸»é¢˜",
        "éå¸¸è¯¦ç»†çš„è¦æ±‚",
        mockResponse
      );

      const endMemory = process.memoryUsage().heapUsed;
      const memoryGrowth = endMemory - startMemory;

      // Memory growth should be reasonable (less than 100MB)
      expect(memoryGrowth).toBeLessThan(100 * 1024 * 1024);
      expect(mockResponse.end).toHaveBeenCalled();
    });

    test("should handle response size limits", async () => {
      const MAX_RESPONSE_SIZE = 50 * 1024; // 50KB limit
      let totalSize = 0;
      let chunkCount = 0;

      mockResponse.write.mockImplementation((chunk) => {
        totalSize += Buffer.byteLength(chunk, 'utf8');
        chunkCount++;
        
        // Simulate response size checking
        if (totalSize > MAX_RESPONSE_SIZE) {
          console.warn(`Response size ${totalSize} exceeds limit ${MAX_RESPONSE_SIZE}`);
        }
      });

      const largeResponseStream = {
        [Symbol.asyncIterator]: async function* () {
          // Generate content that would exceed size limit
          for (let i = 0; i < 2000; i++) {
            yield {
              choices: [{
                delta: { content: "è¿™æ˜¯å¾ˆé•¿çš„æ•™æ¡ˆå†…å®¹ã€‚".repeat(20) }
              }]
            };
          }
        },
      };

      aiService.openai = {
        chat: {
          completions: {
            create: jest.fn().mockResolvedValue(largeResponseStream),
          },
        },
      };

      vectorStore.getRelevantContext.mockResolvedValue({
        context: "æ•™å­¦å†…å®¹",
        sources: ["guide.pdf"],
        totalResults: 1,
        usedResults: 1,
      });

      await aiService.generateLessonPlanStream(
        "æ•°å­¦",
        "ä¸‰å¹´çº§",
        "å°æ•°åŠ æ³•",
        null,
        mockResponse
      );

      // Verify response was processed (even if large)
      expect(chunkCount).toBeGreaterThan(100);
      expect(totalSize).toBeGreaterThan(MAX_RESPONSE_SIZE);
      expect(mockResponse.end).toHaveBeenCalled();
    });
  });

  describe("Unusual Subject and Grade Combinations", () => {
    test("should handle age-inappropriate subject combinations", async () => {
      const inappropriateCombinations = [
        { subject: "ç‰©ç†", grade: "ä¸€å¹´çº§", topic: "é‡å­åŠ›å­¦" },
        { subject: "åŒ–å­¦", grade: "äºŒå¹´çº§", topic: "æœ‰æœºåˆæˆ" },
        { subject: "æ•°å­¦", grade: "å¹¼å„¿å›­", topic: "å¾®ç§¯åˆ†" },
        { subject: "ç”Ÿç‰©", grade: "å­¦å‰ç­", topic: "åŸºå› å·¥ç¨‹" },
      ];

      for (const combo of inappropriateCombinations) {
        // Should throw error for inappropriate combinations
        await expect(
          aiService.generateExercisesStream(
            combo.subject,
            combo.grade,
            combo.topic,
            "easy",
            5,
            "é€‰æ‹©é¢˜",
            null,
            mockResponse
          )
        ).rejects.toThrow();
      }
    });

    test("should handle non-existent subjects gracefully", async () => {
      const nonExistentSubjects = [
        "é­”æ³•å­¦",
        "é£è¡Œæœ¯", 
        "æ—¶é—´æ—…è¡Œ",
        "å¤–æ˜Ÿè¯­è¨€",
        "123456", // Numeric subject
        "!@#$%",  // Special characters
      ];

      vectorStore.getRelevantContext.mockResolvedValue({
        context: "",
        sources: [],
        totalResults: 0,
        usedResults: 0,
      });

      const mockStream = {
        [Symbol.asyncIterator]: async function* () {
          yield {
            choices: [{
              delta: { content: "æ— æ³•å¤„ç†æ­¤ç§‘ç›®çš„æ•™æ¡ˆ" }
            }]
          };
        },
      };

      aiService.openai = {
        chat: {
          completions: {
            create: jest.fn().mockResolvedValue(mockStream),
          },
        },
      };

      for (const subject of nonExistentSubjects) {
        await aiService.generateLessonPlanStream(
          subject,
          "ä¸‰å¹´çº§",
          "åŸºç¡€çŸ¥è¯†",
          null,
          mockResponse
        );

        // Should handle gracefully without crashing
        expect(mockResponse.write).toHaveBeenCalled();
      }
    });
  });

  describe("Failure Recovery Testing", () => {
    test("should recover from partial failures", async () => {
      let callCount = 0;
      
      aiService.openai = {
        chat: {
          completions: {
            create: jest.fn().mockImplementation(() => {
              callCount++;
              if (callCount === 1) {
                // First call fails
                throw new Error("Temporary failure");
              }
              // Second call succeeds
              return Promise.resolve({
                [Symbol.asyncIterator]: async function* () {
                  yield {
                    choices: [{
                      delta: { content: "æ¢å¤åçš„æ•™æ¡ˆå†…å®¹" }
                    }]
                  };
                },
              });
            }),
          },
        },
      };

      vectorStore.getRelevantContext.mockResolvedValue({
        context: "æ•™å­¦å†…å®¹",
        sources: ["guide.pdf"],
        totalResults: 1,
        usedResults: 1,
      });

      // First attempt should fail gracefully
      await aiService.generateLessonPlanStream(
        "æ•°å­¦",
        "ä¸‰å¹´çº§",
        "å°æ•°åŠ æ³•",
        null,
        mockResponse
      );

      expect(mockResponse.write).toHaveBeenCalledWith(
        expect.stringContaining("AIæœåŠ¡é”™è¯¯")
      );
    });
  });
});