# ğŸš€ Complete Self-Hosted TeachAI Docker Deployment

**Everything you need for a fully self-contained TeachAI RAG system with zero external dependencies.**

## ğŸ“‹ What's Included

### âœ… **Complete Self-Hosted Stack**
- **Frontend**: Next.js 15 with React 19
- **Backend**: Node.js Express API server  
- **Database**: MongoDB with optimized indexes
- **Vector Store**: SQLite-VSS (lightweight ChromaDB alternative)
- **Embedding Models**: Self-hosted Sentence Transformers
- **RAG Data**: 1,556 enhanced educational files (232MB, 95k+ chunks)

### âœ… **Zero External Dependencies**
- No ChromaDB Cloud required
- No external embedding APIs
- No external vector databases
- All AI models downloaded and cached locally
- Complete educational dataset included

### âœ… **Resource Optimized**
- **Memory**: 6GB max (perfect for 8GB systems)
- **CPU**: 4-core optimized
- **Storage**: ~3GB total (fits in 100GB easily)
- **Auto-scaling**: Adjusts to available resources

## ğŸš€ One-Command Deployment

```bash
# Complete setup with everything included
./deploy.sh setup
cp .env.docker.example .env
# Edit .env with your API keys
./deploy.sh start
```

## ğŸ“¦ Build Options

### **Option 1: Complete Build (Recommended)**
```bash
./deploy.sh build complete
```
**What you get:**
- âœ… All 1,556 RAG educational files (232MB)
- âœ… 3 self-hosted embedding models pre-downloaded
- âœ… Complete system ready to run offline
- âœ… 15-30 minute build, but everything included
- âœ… Perfect for production deployment

### **Option 2: Fast Build**  
```bash
./deploy.sh build fast
```
**What you get:**
- âœ… Core system components
- âœ… Models download on first use
- âœ… 2-5 minute build time
- âš ï¸ Requires internet on first run

### **Option 3: Auto-Detect**
```bash
./deploy.sh build
```
**What happens:**
- ğŸ¤– Detects if RAG data exists (1,556 files)
- ğŸ¤– Uses complete build if data found
- ğŸ¤– Falls back to fast build otherwise

## ğŸ—ï¸ Architecture Overview

### **Multi-Stage Docker Build**
```
Stage 1: Base Dependencies    (Alpine Linux + Node.js)
Stage 2: Node.js Dependencies (pnpm install)
Stage 3: Frontend Build      (Next.js optimized build)
Stage 4: Server Preparation   (Express + RAG system)
Stage 5: Model Download       (3 embedding models cached)
Stage 6: Production Runtime   (Complete system assembly)
```

### **Self-Hosted Embedding Models**
| Profile | Model | Memory | Use Case |
|---------|-------|--------|----------|
| `ultra-lite` | all-MiniLM-L6-v2 | ~90MB | <3GB RAM systems |
| `balanced` | multilingual-MiniLM-L12-v2 | ~420MB | 4-8GB RAM (recommended) |
| `quality` | multilingual-mpnet-base-v2 | ~1.2GB | >8GB RAM systems |

### **Automatic Resource Detection**
The system automatically:
- âœ… Detects available RAM and selects appropriate model
- âœ… Loads 1,556 educational files on first run
- âœ… Creates optimized vector database
- âœ… Generates embeddings for 95k+ text chunks
- âœ… Sets up search indexes and quality filters

## ğŸ“š Educational Content Included

### **What's in the RAG Database**
- **Files**: 1,556 enhanced JSON files
- **Size**: 232MB of educational content
- **Chunks**: 95,000+ processed text segments
- **Subjects**: è¯­æ–‡, æ•°å­¦, è‹±è¯­, ç‰©ç†, åŒ–å­¦, ç”Ÿç‰©, å†å², åœ°ç†, æ”¿æ²», éŸ³ä¹, ç¾æœ¯, ä½“è‚²
- **Grades**: ä¸€å¹´çº§ through ä¹å¹´çº§ (K-12 coverage)
- **Publishers**: äººæ°‘æ•™è‚²å‡ºç‰ˆç¤¾, åŒ—å¸ˆå¤§ç‰ˆ, é’å²›ç‰ˆ, etc.

### **Content Processing Pipeline**
1. **OCR Correction**: Advanced Chinese text recognition
2. **Quality Scoring**: Reliability metrics for each chunk  
3. **Semantic Features**: Content type classification
4. **Duplicate Detection**: Advanced similarity filtering
5. **Enhancement v2.0**: Latest preprocessing improvements

## ğŸ”§ Configuration Options

### **Environment Variables**
```bash
# Core Configuration
DASHSCOPE_API_KEY=your_qwen_api_key
JWT_SECRET=your_secure_jwt_secret
MONGO_ROOT_PASSWORD=your_mongodb_password

# Self-Hosted RAG Settings
EMBEDDING_PROFILE=balanced     # ultra-lite | balanced | quality
USE_LOCAL_EMBEDDINGS=true
RAG_AUTO_SETUP=true
INIT_LOAD_RAG=true

# Resource Optimization
NODE_OPTIONS=--max-old-space-size=4096
UV_THREADPOOL_SIZE=4
```

### **Automatic Resource Scaling**
The system automatically adjusts based on available resources:

| Available RAM | Auto Profile | Memory Used | Performance |
|---------------|--------------|-------------|-------------|
| <3GB | ultra-lite | ~90MB | Fast, good quality |
| 3-6GB | balanced | ~420MB | Best balance |
| >6GB | quality | ~1.2GB | Highest accuracy |

## ğŸ“Š Expected Performance

### **Build Times**
- **Complete Build**: 15-30 minutes (includes model downloads)
- **Fast Build**: 2-5 minutes (models download on use)
- **Subsequent Builds**: 5-10 minutes (cached layers)

### **Runtime Performance**
- **Cold Start**: 60-120 seconds (model loading + RAG initialization)
- **Warm Start**: 10-30 seconds (database ready)
- **Query Response**: 300-800ms (including vector search)
- **Concurrent Users**: 15-25 (depending on queries)

### **Resource Usage**
```
Container          Memory    CPU     Purpose
teachai-app        4-5GB     2-3     Main application + RAG
teachai-mongodb    800MB     0.5     Database
teachai-nginx      64MB      0.1     Reverse proxy (optional)
Total              ~6GB      3.5     Perfect for 8GB/4-core server
```

## ğŸš€ Deployment Examples

### **Basic Deployment**
```bash
./deploy.sh setup
./deploy.sh start
```

### **Production Deployment with Monitoring**
```bash
./deploy.sh setup
./deploy.sh build complete
./deploy.sh start full
./deploy.sh health
```

### **CentOS Server Deployment**
```bash
# Install Docker
sudo yum install -y docker-ce docker-ce-cli containerd.io
sudo systemctl start docker && sudo systemctl enable docker

# Deploy TeachAI
git clone <repo> teachai && cd teachai
./deploy.sh setup
./deploy.sh build complete
./deploy.sh start production

# Verify
./deploy.sh status
./deploy.sh health
```

## ğŸ” Monitoring & Verification

### **Health Checks**
```bash
# Complete system check
./deploy.sh health

# Individual component checks
docker exec teachai-app ./docker/healthcheck.sh backend
docker exec teachai-app ./docker/healthcheck.sh rag
docker exec teachai-app ./docker/healthcheck.sh resources
```

### **RAG System Status**
```bash
# Check vector database
curl http://localhost:3001/api/rag/status

# Test search functionality
curl -X POST http://localhost:3001/api/rag/search \
  -H "Content-Type: application/json" \
  -d '{"query": "æ•°å­¦", "limit": 3}'

# View container logs
./deploy.sh logs teachai | grep -i rag
```

### **Resource Monitoring**
```bash
# Real-time resource usage
./deploy.sh status

# Detailed container stats
docker stats

# Memory usage breakdown
docker exec teachai-app ./docker/healthcheck.sh resources
```

## ğŸ› ï¸ Troubleshooting

### **Build Issues**
```bash
# Build hanging/failing
docker system prune -f              # Clean Docker cache
./deploy.sh build fast              # Try minimal build
./deploy.sh clean                   # Clean everything

# Out of space during build
docker system df                    # Check space usage
docker image prune -a               # Clean old images
```

### **Runtime Issues**
```bash
# RAG not loading
./deploy.sh logs teachai | grep RAG
docker exec -it teachai-app ls -la /app/server/rag_data/chunks/

# Memory issues
./deploy.sh logs | grep -i memory
# Edit .env: EMBEDDING_PROFILE=ultra-lite
./deploy.sh restart
```

### **Performance Issues**
```bash
# Check system resources
./deploy.sh status

# Monitor embedding service
curl http://localhost:3001/api/health
docker exec teachai-app ps aux | grep python

# Database performance
docker exec teachai-mongodb mongostat --host localhost
```

## ğŸ” Security Considerations

### **Production Security**
- âœ… Non-root container user (teachai:1001)  
- âœ… Network isolation with custom bridge
- âœ… MongoDB not exposed externally
- âœ… Health checks with timeouts
- âœ… Resource limits enforced
- âœ… All secrets via environment variables

### **Firewall Configuration**
```bash
# CentOS/RHEL Firewall
sudo firewall-cmd --permanent --add-port=3000/tcp  # Frontend
sudo firewall-cmd --permanent --add-port=3001/tcp  # API
sudo firewall-cmd --reload
```

## ğŸ“ˆ Scaling Options

### **Vertical Scaling (More Resources)**
```yaml
# docker-compose.yml adjustments
services:
  teachai:
    deploy:
      resources:
        limits:
          memory: 12G        # For 16GB+ servers
          cpus: '7'          # For 8+ core servers
```

### **Data Management**
```bash
# Backup complete system
./deploy.sh backup

# Export RAG data for other instances
docker exec teachai-app sqlite3 /app/data/vectors.db .dump > rag-backup.sql

# Clone to another server
docker save teachai:latest | gzip > teachai-complete.tar.gz
# Transfer and load on new server
```

This complete Docker deployment gives you everything needed for a fully self-hosted TeachAI system with zero external dependencies, optimized for your 4-core/8GB RAM CentOS server constraints while including the full 232MB educational dataset and self-hosted embedding models.