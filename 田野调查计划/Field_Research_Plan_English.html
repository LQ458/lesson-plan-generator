<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Field_Research_Plan_English</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <link rel="stylesheet" href="style.css" />
</head>
<body>
<h1
id="technology-empowering-education-assessment-of-ai-generated-lesson-plans-for-self-study-assistance-in-resource-limited-areas-bijie-guizhou-and-exploration-of-localized-applications">Technology
Empowering Education: Assessment of AI-Generated Lesson Plans for
Self-Study Assistance in Resource-Limited Areas (Bijie, Guizhou) and
Exploration of Localized Applications</h1>
<h2 id="i.-research-background-and-objectives">I. Research Background
and Objectives</h2>
<h3 id="background">Background</h3>
<p>There exists an imbalance in educational resources between urban and
rural areas, as well as between eastern and western regions in China.
Bijie, as a prefecture-level city in Guizhou Province, may have
relatively weak teaching staff and limited access to teaching resources
(especially high-quality electronic devices and network resources) in
some rural schools. Meanwhile, AI technology applications in education
are emerging, but empirical research on their effectiveness in
resource-limited areas is insufficient.</p>
<h3 id="research-objectives">Research Objectives</h3>
<ol type="1">
<li><strong>Empirical Assessment</strong>: Through controlled
experiments, scientifically evaluate the effectiveness differences
between AI-generated lesson plans and traditional lesson plans in
assisting student self-study</li>
<li><strong>Difficulty Insights</strong>: Deeply understand the specific
difficulties local students face in using electronic devices and
accessing resources, providing basis for technology localization</li>
<li><strong>Needs Discovery</strong>: Through field observations, gain
insights into students’ real learning habits, methods, and potential
needs for new learning tools</li>
<li><strong>Product Prototype</strong>: Based on research findings,
design a self-study platform or manual prototype more suitable for
students in Bijie (and similar areas) with low resource dependency</li>
</ol>
<h2 id="ii.-core-research-content-and-methods">II. Core Research Content
and Methods</h2>
<p>This project will adopt a <strong>mixed research approach</strong>
(quantitative + qualitative), divided into four phases:</p>
<h3 id="phase-i-preparation-and-pre-research-1-2-weeks">Phase I:
Preparation and Pre-research (1-2 weeks)</h3>
<p><strong>Main Tasks:</strong> 1. <strong>Contact Research
Sites</strong>: Establish contact with 1-2 target primary and secondary
schools in Bijie, obtaining participation permission from school
administration, teachers, and students 2. <strong>Ethics
Preparation</strong>: Prepare informed consent forms, clearly explaining
research purposes, procedures, privacy protection measures, and their
rights to schools, teachers, students, and parents 3. <strong>Material
Preparation</strong>: Finalize AI lesson plans and traditional lesson
plans for testing (content and difficulty must be strictly matched);
prepare pre-test/post-test questionnaires, interview outlines,
observation record forms, and other tools 4. <strong>Equipment
Inspection</strong>: Conduct on-site inspection of school computer
rooms, availability of students’ own devices, and network conditions,
providing realistic basis for experimental design</p>
<h3 id="phase-ii-quantitative-research---controlled-experiments">Phase
II: Quantitative Research - Controlled Experiments</h3>
<p><strong>Core Measurement Dimensions and Indicators:</strong></p>
<p>We will measure “effectiveness” from four dimensions: 1. Learning
outcomes and efficiency 2. Behavioral engagement 3. Emotional/cognitive
engagement 4. Subjective experience and satisfaction</p>
<h4 id="learning-outcomes-and-efficiency">1. Learning Outcomes and
Efficiency</h4>
<p><strong>Immediate Learning Achievement:</strong> -
<strong>Indicator</strong>: Post-test standardized test scores -
<strong>Measurement Method</strong>: Tests conducted immediately after
the learning phase, including basic knowledge questions and application
questions - <strong>Analysis</strong>: Use independent samples t-test to
compare post-test average scores between AI group and traditional group,
while using analysis of covariance with pre-test scores as
covariates</p>
<p><strong>Learning Efficiency:</strong> - <strong>Indicator</strong>:
Time required to complete specific learning tasks - <strong>Measurement
Method</strong>: Record the time each student spends from starting
self-study to completing all learning materials -
<strong>Analysis</strong>: Use t-test to compare average learning time
between the two groups</p>
<p><strong>Knowledge Retention:</strong> - <strong>Indicator</strong>:
Delayed post-test scores - <strong>Measurement Method</strong>: 1-2
weeks after the first post-test, conduct another test using a
questionnaire similar in difficulty and content to the post-test -
<strong>Analysis</strong>: Compare delayed post-test scores between
groups, evaluating which lesson plan brings more lasting knowledge
memory</p>
<h4 id="behavioral-engagement">2. Behavioral Engagement</h4>
<p><strong>Task Persistence:</strong> - <strong>Indicator</strong>:
Proportion of students who actively give up or fail to complete learning
tasks - <strong>Measurement Method</strong>: Record how many students in
each group stop before completing all materials -
<strong>Analysis</strong>: Compare abandonment rates between groups (can
use chi-square test)</p>
<p><strong>Interaction Frequency:</strong> - <strong>Indicator</strong>:
Number of times actively asking questions or seeking help -
<strong>Measurement Method</strong>: In on-site observation record
forms, record the number of times each student asks content-related
questions to researchers or teachers - <strong>Analysis</strong>:
Compare average question frequency between groups (t-test)</p>
<p><strong>Note-taking Behavior:</strong> - <strong>Indicator</strong>:
Proportion of students taking notes and word count/volume of notes -
<strong>Measurement Method</strong>: Observe and record how many
students in each group take notes, collect students’ note papers for
word count or content analysis - <strong>Analysis</strong>: Compare
note-taking behavior differences between groups</p>
<h4 id="emotional-and-cognitive-engagement">3. Emotional and Cognitive
Engagement</h4>
<p><strong>Cognitive Load:</strong> - <strong>Indicator</strong>:
NASA-TLX cognitive load scale (simplified version) scores -
<strong>Measurement Method</strong>: After learning, have students
answer several questions on a 9-point scale - <strong>Analysis</strong>:
Compare total cognitive load scores between groups, lower scores
indicate smoother learning process</p>
<p><strong>Situational Interest:</strong> - <strong>Indicator</strong>:
Immediate interest level - <strong>Measurement Method</strong>: After
learning, directly ask: “How interesting do you find the learning
content just covered?” (using 1-5 point Likert scale) -
<strong>Analysis</strong>: Compare average interest scores between
groups</p>
<h4 id="subjective-experience-and-satisfaction">4. Subjective Experience
and Satisfaction</h4>
<p><strong>Lesson Plan Satisfaction Questionnaire:</strong> -
<strong>Indicator</strong>: Satisfaction scale scores -
<strong>Measurement Method</strong>: Use a questionnaire with multiple
items, having students evaluate the clarity, usefulness, attractiveness,
etc. of lesson plans - <strong>Analysis</strong>: Calculate average
scores for each dimension and conduct between-group comparisons</p>
<p><strong>Preference Choice:</strong> - <strong>Indicator</strong>:
Future usage intention - <strong>Measurement Method</strong>: At the end
of the experiment, directly ask students which material they prefer to
use in the future - <strong>Analysis</strong>: Count and compare
proportions of students choosing different lesson plans (chi-square
test)</p>
<p><strong>Data Collection Tools:</strong> 1. Pre-test/post-test/delayed
post-test questionnaires: Carefully designed standardized questionnaires
2. Observation record forms: Structured forms for recording behavioral
indicators 3. Immediate feedback scales: NASA-TLX simplified scale and
situational interest single-question scale 4. Satisfaction
questionnaire: Short questionnaire with 5-7 items 5. Timer: For
recording each student’s learning time</p>
<h3
id="phase-iii-qualitative-research---field-observations-and-in-depth-interviews">Phase
III: Qualitative Research - Field Observations and In-depth
Interviews</h3>
<p><strong>Core Objectives:</strong> 1. <strong>Deep Understanding of
Context</strong>: Reveal reasons behind numbers, explaining causes of
experimental results 2. <strong>Discover Unexpected Insights</strong>:
Capture subtle behaviors and attitudes that quantitative questionnaires
cannot cover 3. <strong>Insight into Real Needs</strong>: Provide
first-hand inspiration for designing truly “usable and user-friendly”
localized self-study solutions</p>
<p><strong>Preparation Work:</strong> 1. <strong>Ethics First</strong>:
Prepare and sign informed consent forms, clearly informing research
purposes, data usage, privacy protection measures, and right to withdraw
at any time 2. <strong>Tool Preparation</strong>: Observation record
forms, interview outlines, notebooks, pens, voice recorders (with
permission), cameras (with permission) 3. <strong>Clear
Identity</strong>: Inform that the role is “learner” and “observer,” not
“evaluator,” to reduce “Hawthorne effect”</p>
<h4 id="participatory-observation-guidelines">Participatory Observation
Guidelines</h4>
<p><strong>Observation Focus</strong>: Behavior, interaction,
environment, emotional responses</p>
<p><strong>Observation Content Checklist:</strong></p>
<ol type="1">
<li><strong>Learning Habits and Processes:</strong>
<ul>
<li>Pre-class/self-study start: How do they prepare? Do they actively
preview? How do they obtain materials?</li>
<li>Note-taking methods: Do they copy from blackboard/screen, or have
their own methods (like underlining, drawing, summarizing)? Where do
they write notes (dedicated notebooks, books, scrap paper)?</li>
<li>When encountering difficulties: What’s the first reaction? (staring
blankly, flipping through books, asking desk mates, asking teachers,
giving up directly) How do they express questions?</li>
<li>Tool usage: Besides textbooks, what else do they use? Reference
books? Dictionaries? Phones? How do they use them?</li>
</ul></li>
<li><strong>Reactions to Experimental Materials (AI
vs. Traditional):</strong>
<ul>
<li>Confusion points: Record specifically which knowledge point or
sentence caused confusion</li>
<li>Excitement points/interest points: Record what content sparked
interest</li>
<li>Operational difficulties: Observe if they get stuck on interface
navigation? Do they get distracted due to device lag or low
battery?</li>
</ul></li>
<li><strong>Social Interaction Patterns:</strong>
<ul>
<li>Is learning isolated or collaborative? How do they discuss
problems?</li>
<li>Student-teacher interaction patterns</li>
</ul></li>
<li><strong>Environment and Context:</strong>
<ul>
<li>Classroom lighting, noise, desk and chair layout</li>
<li>Is electronic device charging convenient? How is network
signal?</li>
</ul></li>
</ol>
<p><strong>Observation Methods:</strong> - “Umbrella” observation: First
conduct overall scanning to understand overall class atmosphere -
“Focus” observation: Select 3-5 representative students for 15-minute
interval tracking records - Momentary sampling: Every 5 minutes, quickly
record current moment behaviors of specific students</p>
<p><strong>Observation Record Form Template:</strong></p>
<table style="width:100%;">
<colgroup>
<col style="width: 4%" />
<col style="width: 32%" />
<col style="width: 34%" />
<col style="width: 29%" />
</colgroup>
<thead>
<tr>
<th>Time</th>
<th>Observed Behavior/Event (Objective Description)</th>
<th>My Interpretation/Questions (Subjective Thinking)</th>
<th>Possible Corresponding Research Questions</th>
</tr>
</thead>
<tbody>
<tr>
<td>14:05</td>
<td>Student A saw the animated illustration in the lesson plan, nudged
Student B with elbow, pointed at screen, whispered “Wow, this is
cool”</td>
<td>Dynamic visual content effectively stimulates interest. Traditional
lesson plans lack this element</td>
<td>AI lesson plans’ advantages in boosting interest?</td>
</tr>
<tr>
<td>14:15</td>
<td>Student C stayed at page 3 exercises for over 5 minutes, constantly
erasing and rewriting, finally sighed and put down pen</td>
<td>This question might be too difficult, or explanation insufficient.
Clear frustration. Need to ask during interview</td>
<td>Is AI lesson plan’s difficulty gradient setting reasonable?</td>
</tr>
<tr>
<td>14:30</td>
<td>Over half the students tried clicking on images in printed lesson
plan, seemingly expecting interactive effects</td>
<td>Students are already accustomed to digital media interactivity,
paper materials may seem “rigid”</td>
<td>How to combine offline and online forms?</td>
</tr>
</tbody>
</table>
<h4 id="semi-structured-interview-guidelines">Semi-structured Interview
Guidelines</h4>
<p><strong>Core Principles</strong>: Ask more open-ended “why” and “how”
questions, avoid leading questions, create relaxed, trusting
atmosphere</p>
<p><strong>Student Interview Outline:</strong></p>
<ol type="1">
<li><strong>Background and Resource Access:</strong>
<ul>
<li>“Do you have computers or tablets at home that can be used for
learning? Who mainly uses them?”</li>
<li>“What do you usually do with your phone? Do you use it to look up
study materials? How?”</li>
<li>“When you encounter homework problems, besides asking teachers and
classmates, what else do you do?”</li>
</ul></li>
<li><strong>Learning Experience and Difficulties:</strong>
<ul>
<li>“For today’s new (AI) material, which part did you find most
difficult? Why?”</li>
<li>“Was there any part that made you think ‘Oh, that’s how it is’ or
found interesting?”</li>
<li>“Compared to lesson plans teachers usually give, what do you think
is the biggest difference? Which do you prefer? Why?”</li>
</ul></li>
<li><strong>Imagination and Preferences:</strong>
<ul>
<li>“If you were to design a tool to help you self-study, what would you
want it to look like?”</li>
<li>“What functions would you hope it has?”</li>
</ul></li>
</ol>
<p><strong>Teacher Interview Outline:</strong></p>
<ol type="1">
<li><strong>Teaching Practice:</strong>
<ul>
<li>“How do you usually prepare lessons and lesson plans/study guides?
What resources do you typically reference?”</li>
<li>“In your view, what are the biggest difficulties students face in
self-study?”</li>
</ul></li>
<li><strong>Technology Attitudes and Challenges:</strong>
<ul>
<li>“How do you view AI technology assisting teaching? What expectations
and concerns do you have?”</li>
<li>“What practical difficulties do you encounter using digital
technology in teaching?”</li>
</ul></li>
<li><strong>Feedback on Experiments:</strong>
<ul>
<li>“You observed students using these materials, what impressed you
most?”</li>
<li>“Where do you think AI-generated lesson plans might really help you?
Where might not be very applicable?”</li>
</ul></li>
</ol>
<h4 id="artifact-collection-and-analysis">Artifact Collection and
Analysis</h4>
<p><strong>Collection Content</strong>: Notes, draft papers, homework
produced by students during experiments</p>
<p><strong>Analysis Methods</strong>: - <strong>Content</strong>: What
did they record? Key points, example questions, or their own summaries?
- <strong>Structure</strong>: Are notes organized? Can they reflect
their thinking logic? - <strong>Interaction with Materials</strong>: Did
they do extensive marking, annotation on materials?</p>
<p><strong>Daily Field Reflection</strong>: After each day, spend 30
minutes answering these questions: 1. What was the biggest surprise
today? 2. Were my main assumptions challenged? 3. Did I overlook any
group or perspective? 4. How should I adjust tomorrow’s observations and
interviews?</p>
<h3
id="phase-iv-comprehensive-analysis-and-product-design-1-2-weeks-after-research">Phase
IV: Comprehensive Analysis and Product Design (1-2 weeks after
research)</h3>
<p><strong>Main Tasks</strong>: Self-study manual &amp;
Platform/website/manual prototype</p>
<p><strong>Methods</strong>: 1. <strong>Data Triangulation</strong>:
Cross-validate quantitative data with qualitative data for comprehensive
analysis, explaining “why” such results were produced 2. <strong>Design
Thinking Workshop</strong>: Invite some students and teachers to
brainstorm what ideal self-study tools should look like based on
discovered needs and difficulties 3. <strong>Output Results</strong>: -
<strong>Field Research Report</strong>: Detailed elaboration of research
findings, conclusions, and recommendations - <strong>“Low Resource
Dependency” Self-study Manual Prototype</strong>: Imitate Khan Academy
style but fully consider Bijie’s actual situation: - Offline priority:
Content can be printed into booklets or support offline download in
mobile APP form - Lightweight: Compressed images and videos to reduce
traffic consumption - Extremely simple operation: Clean interface, clear
steps, lowering usage threshold - Localized cases: Integrate local
student-familiar contexts and examples in example questions and
explanations - <strong>Localization Recommendations</strong>: Analyze
feasibility of directly transplanting high-end platforms, propose how to
take their essence while removing dross, achieving dimensional reduction
application strategies</p>
<h2 id="iii.-timeline">III. Timeline</h2>
<table>
<colgroup>
<col style="width: 24%" />
<col style="width: 28%" />
<col style="width: 48%" />
</colgroup>
<thead>
<tr>
<th>Week</th>
<th>Phase</th>
<th>Main Tasks</th>
</tr>
</thead>
<tbody>
<tr>
<td>Week 1</td>
<td>Preparation and Pre-research</td>
<td>Contact schools, obtain permission, prepare materials, ethics
review</td>
</tr>
<tr>
<td>Week 2</td>
<td>Quantitative Experiments</td>
<td>Implement pre-test, intervention, post-test, collect quantitative
data</td>
</tr>
<tr>
<td>Week 3</td>
<td>Field Observations</td>
<td>Conduct classroom observations, carry out student/teacher
interviews</td>
</tr>
<tr>
<td>Week 4</td>
<td>Data Analysis</td>
<td>Data organization, statistical analysis, interview
transcription</td>
</tr>
<tr>
<td>Week 5-6</td>
<td>Report Writing and Design</td>
<td>Synthesize all data, write report, design self-study manual
prototype</td>
</tr>
</tbody>
</table>
<h2 id="iv.-expected-outcomes">IV. Expected Outcomes</h2>
<ol type="1">
<li><strong>Academic Outcomes</strong>: A complete field research report
that can be used for course assignments, academic conferences, or
journal publications</li>
<li><strong>Practical Outcomes</strong>: A field-validated “AI-assisted
lesson plan” application model and localized self-study manual prototype
applicable to resource-limited areas</li>
<li><strong>Social Value</strong>: Provide empirical evidence and
implementation references for educational technology companies,
non-profit organizations, and government departments promoting
educational technology in similar areas</li>
</ol>
<h2 id="v.-potential-challenges-and-responses">V. Potential Challenges
and Responses</h2>
<p><strong>Challenge 1: Difficulty in School Access</strong> -
<strong>Response</strong>: Connect through local education bureaus,
Communist Youth League, university teaching support associations,
emphasizing research’s public welfare nature and value to schools</p>
<p><strong>Challenge 2: Extremely Poor Electronic Devices and Network
Conditions</strong> - <strong>Response</strong>: Prepare offline
solutions as backup (such as printed lesson plans, videos stored on
local computers). This is exactly the “difficulty” that research needs
to focus on</p>
<p><strong>Challenge 3: Low Student/Teacher Participation</strong> -
<strong>Response</strong>: Design interesting learning content and
interactive sessions; prepare some small gifts (stationery, etc.) as
thanks</p>
<p><strong>Challenge 4: Cultural Differences and Understanding
Deviations</strong> - <strong>Response</strong>: Find a local person as
guide or translator; maintain open and humble learning attitude, avoid
preconceptions</p>
<h2 id="vi.-required-resources-and-budget">VI. Required Resources and
Budget</h2>
<p><strong>Human Resources</strong>: 1-2 researchers, possible local
coordinator or assistant</p>
<p><strong>Material Resources</strong>: Test questionnaires, interview
outlines, notebooks, voice recorders (with permission), thank you
gifts</p>
<p><strong>Technical Resources</strong>: Laptops, electronic devices for
testing (if schools cannot provide), statistical analysis software (such
as SPSS, Python, R)</p>
<p><strong>Budget</strong>: Including transportation, accommodation,
material printing, gifts, possible service fees, etc.</p>
</body>
</html>
